{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    " # ----------- Overhead -------------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import scipy.stat as sps\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# from pandas import ExcelWriter\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "import numpy as np\n",
    "\n",
    "## Fix roger's stuff\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('NY property csv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------- Fill in NAs and 0s ----------\n",
    "\n",
    "# Create new variable BORO\n",
    "df['BORO'] = ''\n",
    "df['BORO'] = df.BBLE.str[0]\n",
    "\n",
    "# ZIP, sort by BBLE and forward fill\n",
    "df = df.sort_values(by = ['BBLE'])\n",
    "df['ZIP'].fillna(method='ffill',inplace = True)\n",
    "\n",
    "# Create ZIP3\n",
    "df['ZIP3']= df[\"ZIP\"].apply(lambda x: int(str(x)[0:3]))\n",
    "\n",
    "\n",
    "df['ll_rate'] = df['LTFRONT']/df['LTDEPTH']\n",
    "mean_llrate = np.mean(df['ll_rate'].dropna())\n",
    "df['LTFRONT'] = df['LTFRONT'].fillna(value = (df['LTDEPTH'] * mean_llrate))\n",
    "df['LTFRONT'] = df['LTDEPTH'].fillna(value = (df['LTFRONT'] / mean_llrate))\n",
    "np.std(df['LTFRONT'])\n",
    "# std = 73.7 without group\n",
    "# two methods to set 0 values as missing values\n",
    "df.loc[df['LTFRONT'] == 0, 'LTFRONT'] = np.nan\n",
    "# df['LTFRONT'] = df['LTFRONT'].replace(0,np.nan)\n",
    "\n",
    "# fill NA with average values grouped by BORO (if using TAXCLASS as well, \n",
    "# the filled values will be too big and there will be 2 missing values)\n",
    "df['LTFRONT'] = df.groupby(['BORO','TAXCLASS'])['LTFRONT'].transform(lambda x: x.fillna(x.mean()))\n",
    "df['LTFRONT'] = df.groupby(['TAXCLASS'])['LTFRONT'].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "\n",
    "df.loc[df['LTDEPTH'] == 0, 'LTDEPTH'] = np.nan\n",
    "df['LTDEPTH'] = df.groupby(['BORO','TAXCLASS'])['LTDEPTH'].transform(lambda x: x.fillna(x.mean()))\n",
    "df['LTDEPTH'] = df.groupby(['TAXCLASS'])['LTDEPTH'].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "# np.isnan(df['LTDEPTH']).sum()\n",
    "\n",
    "# Calculate BLDFRONT/BLDDEPTH ratio\n",
    "df.loc[df['BLDFRONT'] == 0, 'BLDFRONT'] = np.nan\n",
    "df.loc[df['BLDDEPTH'] == 0, 'BLDDEPTH'] = np.nan\n",
    "\n",
    "df['bb_rate'] = df['BLDFRONT']/df['BLDDEPTH']\n",
    "# distribution of the ratio\n",
    "# sns.distplot(df['BLDFRONT']/df['BLDDEPTH'].dropna(), bins = 1000,kde=False)\n",
    "# plt.xlim(0,2)\n",
    "\n",
    "median_bbrate = np.median(df['bb_rate'].dropna())\n",
    "df['BLDFRONT'] = df['BLDFRONT'].fillna(value = (df['BLDDEPTH'] * median_bbrate))\n",
    "df['BLDDEPTH'] = df['BLDDEPTH'].fillna(value = (df['BLDFRONT'] / median_bbrate))\n",
    "\n",
    "\n",
    "# For BLDFRONT and BLDDEPTH, if group by both BORO and TAXCLASS, there will appear NaN as well.\n",
    "df['BLDFRONT'] = df.groupby(['BORO','TAXCLASS'])['BLDFRONT'].transform(lambda x: x.fillna(x.mean()))\n",
    "df['BLDFRONT'] = df.groupby(['TAXCLASS'])['BLDFRONT'].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "# np.isnan(df['BLDFRONT']).sum()\n",
    "df['BLDDEPTH'] = df.groupby(['BORO','TAXCLASS'])['BLDDEPTH'].transform(lambda x: x.fillna(x.mean()))\n",
    "df['BLDDEPTH'] = df.groupby(['TAXCLASS'])['BLDDEPTH'].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "np.isnan(df['BLDFRONT']).sum()\n",
    "np.isnan(df['BLDDEPTH']).sum()\n",
    "\n",
    "df['STORIES'] = df.groupby(['ZIP','TAXCLASS'])['STORIES'].transform(lambda x: x.fillna(x.mean()))\n",
    "df['STORIES'] = df.groupby(['TAXCLASS'])['STORIES'].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "# np.isnan(df['STORIES']).sum()\n",
    "\n",
    "# building volume and bins accordingly\n",
    "df['bldvol'] = df['BLDFRONT'] * df['BLDDEPTH'] * df['STORIES']\n",
    "\n",
    "df['bldvol_bin'] = pd.qcut(df['bldvol'], 100, labels = False, duplicates = 'drop')\n",
    "\n",
    "# FULLVAL\n",
    "df.loc[df['FULLVAL'] == 0, 'FULLVAL'] = np.nan\n",
    "\n",
    "# df['FULLVAL']\n",
    "df['FULLVAL'] = df.groupby(['BORO','bldvol_bin'])['FULLVAL'].transform(lambda x: x.fillna(x.mean()))\n",
    "# np.isnan(df['FULLVAL']).sum()\n",
    "\n",
    "\n",
    "# AVLAND\n",
    "df.loc[df['AVLAND'] == 0, 'AVLAND'] = np.nan\n",
    "df['AVLAND'] = df.groupby(['BORO','bldvol_bin'])['AVLAND'].transform(lambda x: x.fillna(x.mean()))\n",
    "# np.isnan(df['AVLAND']).sum()\n",
    "\n",
    "\n",
    "# AVTOT\n",
    "df.loc[df['AVTOT'] == 0, 'AVTOT'] = np.nan\n",
    "df['AVTOT'] = df.groupby(['BORO','bldvol_bin'])['AVTOT'].transform(lambda x: x.fillna(x.mean()))\n",
    "# np.isnan(df['AVTOT']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build 3 sizes\n",
    "## Build LotArea, Building Area, Building Volume\n",
    "df['lotarea'] = df['LTDEPTH'] * df['LTFRONT']\n",
    "df['bldarea'] = df['BLDFRONT'] * df['BLDDEPTH']\n",
    "df['bldvol'] = df['bldarea'] * df['STORIES']\n",
    "\n",
    "## Build 9 values\n",
    "## FV = FULLVAL, AL = AVLAND, AT = AVTOT\n",
    "## LA = LOTAREA, BA = BLDAREA, BV = BLDVOL\n",
    "df['fv_la'] = df['FULLVAL'] / df['lotarea']\n",
    "df['fv_ba'] = df['FULLVAL'] / df['bldarea']\n",
    "df['fv_bv'] = df['FULLVAL'] / df['bldvol']\n",
    "df['al_la'] = df['AVLAND'] / df['lotarea']\n",
    "df['al_ba'] = df['AVLAND'] / df['bldarea']\n",
    "df['al_bv'] = df['AVLAND'] / df['bldvol']\n",
    "df['at_la'] = df['AVTOT'] / df['lotarea']\n",
    "df['at_ba'] = df['AVTOT'] / df['bldarea']\n",
    "df['at_bv'] = df['AVTOT'] / df['bldvol']\n",
    "\n",
    "## Build 45 Group by values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ninevars = ['fv_la','fv_ba','fv_bv','al_la','al_ba','al_bv','at_la','at_ba','at_bv']\n",
    "df = df.join(df.groupby(['ZIP'])[ninevars].mean(), on='ZIP', rsuffix='_zip')\n",
    "df = df.join(df.groupby(['ZIP3'])[ninevars].mean(), on='ZIP3', rsuffix='_zip3')\n",
    "df = df.join(df.groupby(['TAXCLASS'])[ninevars].mean(), on='TAXCLASS', rsuffix='_taxclass')\n",
    "df = df.join(df.groupby(['BORO'])[ninevars].mean(), on='BORO', rsuffix='_boro')\n",
    "# add stories bins\n",
    "df['story_bin'] = pd.qcut(df['STORIES'], 10, labels=False, duplicates = 'drop')\n",
    "df = df.join(df.groupby(['story_bin'])[ninevars].mean(), on='story_bin', rsuffix='_story')\n",
    "df = df.join(df.groupby(['BORO','TAXCLASS'])[ninevars].mean(), on=['BORO','TAXCLASS'], rsuffix='_BoroTax')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = df[['fv_la',\n",
    " 'fv_ba',\n",
    " 'fv_bv',\n",
    " 'al_la',\n",
    " 'al_ba',\n",
    " 'al_bv',\n",
    " 'at_la',\n",
    " 'at_ba',\n",
    " 'at_bv',\n",
    " 'fv_la_zip',\n",
    " 'fv_ba_zip',\n",
    " 'fv_bv_zip',\n",
    " 'al_la_zip',\n",
    " 'al_ba_zip',\n",
    " 'al_bv_zip',\n",
    " 'at_la_zip',\n",
    " 'at_ba_zip',\n",
    " 'at_bv_zip',\n",
    " 'fv_la_zip3',\n",
    " 'fv_ba_zip3',\n",
    " 'fv_bv_zip3',\n",
    " 'al_la_zip3',\n",
    " 'al_ba_zip3',\n",
    " 'al_bv_zip3',\n",
    " 'at_la_zip3',\n",
    " 'at_ba_zip3',\n",
    " 'at_bv_zip3',\n",
    " 'fv_la_taxclass',\n",
    " 'fv_ba_taxclass',\n",
    " 'fv_bv_taxclass',\n",
    " 'al_la_taxclass',\n",
    " 'al_ba_taxclass',\n",
    " 'al_bv_taxclass',\n",
    " 'at_la_taxclass',\n",
    " 'at_ba_taxclass',\n",
    " 'at_bv_taxclass',\n",
    " 'fv_la_boro',\n",
    " 'fv_ba_boro',\n",
    " 'fv_bv_boro',\n",
    " 'al_la_boro',\n",
    " 'al_ba_boro',\n",
    " 'al_bv_boro',\n",
    " 'at_la_boro',\n",
    " 'at_ba_boro',\n",
    " 'at_bv_boro',\n",
    " 'fv_la_story',\n",
    " 'fv_ba_story',\n",
    " 'fv_bv_story',\n",
    " 'al_la_story',\n",
    " 'al_ba_story',\n",
    " 'al_bv_story',\n",
    " 'at_la_story',\n",
    " 'at_ba_story',\n",
    " 'at_bv_story',\n",
    " 'fv_la_BoroTax',\n",
    " 'fv_ba_BoroTax',\n",
    " 'fv_bv_BoroTax',\n",
    " 'al_la_BoroTax',\n",
    " 'al_ba_BoroTax',\n",
    " 'al_bv_BoroTax',\n",
    " 'at_la_BoroTax',\n",
    " 'at_ba_BoroTax',\n",
    " 'at_bv_BoroTax']]\n",
    "\n",
    "# newdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Z scale to prepare for dimensionality reduction ----\n",
    "\n",
    "# Create the Scaler object\n",
    "scaler = preprocessing.StandardScaler()\n",
    "# Fit your data on the scaler object\n",
    "scaled_df = scaler.fit_transform(newdf)\n",
    "scaled_df = pd.DataFrame(scaled_df, columns=list(newdf))\n",
    "# scaled_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---- PCA: reduce dimensions ----\n",
    "pca = PCA(n_components = .85, svd_solver = 'full')\n",
    "pca.fit(scaled_df)\n",
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25435587 0.15936895 0.09257827 0.07293785 0.05871896 0.04988091\n",
      " 0.04670502 0.04424925 0.04028856 0.03322871]\n"
     ]
    }
   ],
   "source": [
    "# Percentage of variance explained by each of the selected components.\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformed dataframe\n",
    "pca_df = pd.DataFrame(data = pca.transform(scaled_df), columns =  [\"PC\" + str(i) for i in range(1, pca.n_components_+1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Z scale to PCA dataframe ----\n",
    "\n",
    "# Create the Scaler object\n",
    "scaler = preprocessing.StandardScaler()\n",
    "# Fit your data on the scaler object\n",
    "scaled_pca = scaler.fit_transform(pca_df)\n",
    "scaled_pca = pd.DataFrame(scaled_pca, columns=list(pca_df))\n",
    "# scaled_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Convert df to ndarray\n",
    "matrix_pca = scaled_pca.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "   2288/1048575 [..............................] - ETA: 48:24 - loss: 0.5772 ETA: 49:1"
     ]
    }
   ],
   "source": [
    "## Autoencoder\n",
    "def simple_auto_encode(input_matrix, compression_factor=1.1, encode_activation='tanh', decode_activation='linear',\n",
    "                       optimizer='Adadelta', loss='mean_squared_error'):\n",
    "    _, input_dim = matrix_pca.shape\n",
    "    encoding_dim = int(input_dim // compression_factor)\n",
    "\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    encode_layer = Dense(encoding_dim, activation=encode_activation)(input_layer)\n",
    "    decode_layer = Dense(input_dim, activation=decode_activation)(encode_layer)\n",
    "\n",
    "    auto_encoder = Model(input_layer, decode_layer)\n",
    "    auto_encoder.compile(optimizer=optimizer, loss=loss)\n",
    "    auto_encoder.fit(input_matrix, input_matrix, epochs=10, batch_size=1)\n",
    "\n",
    "    return auto_encoder.predict(input_matrix)\n",
    "\n",
    "\n",
    "matrix_reproduce = simple_auto_encode(matrix_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix_reproduce = np.loadtxt('data/reproduce.txt', dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = ((matrix_pca - matrix_reproduce)**2).mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(mse, bins=100, range=(0,0.001))\n",
    "plt.show()\n",
    "## Mse has the most ideal distribution. Therefore we use mse as fraud score 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurement = (((matrix_pca - matrix_reproduce)**2).sum(axis=1))**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(measurement, bins=100, range=(0,0.1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_pca[\"mae\"] = (matrix_pca - matrix_reproduce).mean(axis=1)\n",
    "scaled_pca[\"MAHAL\"]=(scaled_pca[\"PC1\"]**2+scaled_pca[\"PC2\"]**2+scaled_pca[\"PC3\"]**2+scaled_pca[\"PC4\"]**2+scaled_pca[\"PC5\"]**2+scaled_pca[\"PC6\"]**2+scaled_pca[\"PC7\"]**2+scaled_pca[\"PC8\"]**2+scaled_pca[\"PC9\"]**2+scaled_pca[\"PC10\"]**2)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(mae, bins=100, range=(0,0.1))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
