{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    " # ----------- Overhead -------------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import scipy.stat as sps\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# from pandas import ExcelWriter\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "import numpy as np\n",
    "\n",
    "## Fix roger's stuff\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('NY property csv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------- Fill in NAs and 0s ----------\n",
    "\n",
    "# Create new variable BORO\n",
    "df['BORO'] = ''\n",
    "df['BORO'] = df.BBLE.str[0]\n",
    "\n",
    "# ZIP, sort by BBLE and forward fill\n",
    "df = df.sort_values(by = ['BBLE'])\n",
    "df['ZIP'].fillna(method='ffill',inplace = True)\n",
    "\n",
    "# Create ZIP3\n",
    "df['ZIP3']= df[\"ZIP\"].apply(lambda x: int(str(x)[0:3]))\n",
    "\n",
    "\n",
    "df['ll_rate'] = df['LTFRONT']/df['LTDEPTH']\n",
    "mean_llrate = np.mean(df['ll_rate'].dropna())\n",
    "df['LTFRONT'] = df['LTFRONT'].fillna(value = (df['LTDEPTH'] * mean_llrate))\n",
    "df['LTFRONT'] = df['LTDEPTH'].fillna(value = (df['LTFRONT'] / mean_llrate))\n",
    "np.std(df['LTFRONT'])\n",
    "# std = 73.7 without group\n",
    "# two methods to set 0 values as missing values\n",
    "df.loc[df['LTFRONT'] == 0, 'LTFRONT'] = np.nan\n",
    "# df['LTFRONT'] = df['LTFRONT'].replace(0,np.nan)\n",
    "\n",
    "# fill NA with average values grouped by BORO (if using TAXCLASS as well, \n",
    "# the filled values will be too big and there will be 2 missing values)\n",
    "df['LTFRONT'] = df.groupby(['BORO','TAXCLASS'])['LTFRONT'].transform(lambda x: x.fillna(x.mean()))\n",
    "df['LTFRONT'] = df.groupby(['TAXCLASS'])['LTFRONT'].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "\n",
    "df.loc[df['LTDEPTH'] == 0, 'LTDEPTH'] = np.nan\n",
    "df['LTDEPTH'] = df.groupby(['BORO','TAXCLASS'])['LTDEPTH'].transform(lambda x: x.fillna(x.mean()))\n",
    "df['LTDEPTH'] = df.groupby(['TAXCLASS'])['LTDEPTH'].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "# np.isnan(df['LTDEPTH']).sum()\n",
    "\n",
    "# Calculate BLDFRONT/BLDDEPTH ratio\n",
    "df.loc[df['BLDFRONT'] == 0, 'BLDFRONT'] = np.nan\n",
    "df.loc[df['BLDDEPTH'] == 0, 'BLDDEPTH'] = np.nan\n",
    "\n",
    "df['bb_rate'] = df['BLDFRONT']/df['BLDDEPTH']\n",
    "# distribution of the ratio\n",
    "# sns.distplot(df['BLDFRONT']/df['BLDDEPTH'].dropna(), bins = 1000,kde=False)\n",
    "# plt.xlim(0,2)\n",
    "\n",
    "median_bbrate = np.median(df['bb_rate'].dropna())\n",
    "df['BLDFRONT'] = df['BLDFRONT'].fillna(value = (df['BLDDEPTH'] * median_bbrate))\n",
    "df['BLDDEPTH'] = df['BLDDEPTH'].fillna(value = (df['BLDFRONT'] / median_bbrate))\n",
    "\n",
    "\n",
    "# For BLDFRONT and BLDDEPTH, if group by both BORO and TAXCLASS, there will appear NaN as well.\n",
    "df['BLDFRONT'] = df.groupby(['BORO','TAXCLASS'])['BLDFRONT'].transform(lambda x: x.fillna(x.mean()))\n",
    "df['BLDFRONT'] = df.groupby(['TAXCLASS'])['BLDFRONT'].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "# np.isnan(df['BLDFRONT']).sum()\n",
    "df['BLDDEPTH'] = df.groupby(['BORO','TAXCLASS'])['BLDDEPTH'].transform(lambda x: x.fillna(x.mean()))\n",
    "df['BLDDEPTH'] = df.groupby(['TAXCLASS'])['BLDDEPTH'].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "np.isnan(df['BLDFRONT']).sum()\n",
    "np.isnan(df['BLDDEPTH']).sum()\n",
    "\n",
    "df['STORIES'] = df.groupby(['ZIP','TAXCLASS'])['STORIES'].transform(lambda x: x.fillna(x.mean()))\n",
    "df['STORIES'] = df.groupby(['TAXCLASS'])['STORIES'].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "# np.isnan(df['STORIES']).sum()\n",
    "\n",
    "# building volume and bins accordingly\n",
    "df['bldvol'] = df['BLDFRONT'] * df['BLDDEPTH'] * df['STORIES']\n",
    "\n",
    "df['bldvol_bin'] = pd.qcut(df['bldvol'], 100, labels = False, duplicates = 'drop')\n",
    "\n",
    "# FULLVAL\n",
    "df.loc[df['FULLVAL'] == 0, 'FULLVAL'] = np.nan\n",
    "\n",
    "# df['FULLVAL']\n",
    "df['FULLVAL'] = df.groupby(['BORO','bldvol_bin'])['FULLVAL'].transform(lambda x: x.fillna(x.mean()))\n",
    "# np.isnan(df['FULLVAL']).sum()\n",
    "\n",
    "\n",
    "# AVLAND\n",
    "df.loc[df['AVLAND'] == 0, 'AVLAND'] = np.nan\n",
    "df['AVLAND'] = df.groupby(['BORO','bldvol_bin'])['AVLAND'].transform(lambda x: x.fillna(x.mean()))\n",
    "# np.isnan(df['AVLAND']).sum()\n",
    "\n",
    "\n",
    "# AVTOT\n",
    "df.loc[df['AVTOT'] == 0, 'AVTOT'] = np.nan\n",
    "df['AVTOT'] = df.groupby(['BORO','bldvol_bin'])['AVTOT'].transform(lambda x: x.fillna(x.mean()))\n",
    "# np.isnan(df['AVTOT']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build 3 sizes\n",
    "## Build LotArea, Building Area, Building Volume\n",
    "df['lotarea'] = df['LTDEPTH'] * df['LTFRONT']\n",
    "df['bldarea'] = df['BLDFRONT'] * df['BLDDEPTH']\n",
    "df['bldvol'] = df['bldarea'] * df['STORIES']\n",
    "\n",
    "## Build 9 values\n",
    "## FV = FULLVAL, AL = AVLAND, AT = AVTOT\n",
    "## LA = LOTAREA, BA = BLDAREA, BV = BLDVOL\n",
    "df['fv_la'] = df['FULLVAL'] / df['lotarea']\n",
    "df['fv_ba'] = df['FULLVAL'] / df['bldarea']\n",
    "df['fv_bv'] = df['FULLVAL'] / df['bldvol']\n",
    "df['al_la'] = df['AVLAND'] / df['lotarea']\n",
    "df['al_ba'] = df['AVLAND'] / df['bldarea']\n",
    "df['al_bv'] = df['AVLAND'] / df['bldvol']\n",
    "df['at_la'] = df['AVTOT'] / df['lotarea']\n",
    "df['at_ba'] = df['AVTOT'] / df['bldarea']\n",
    "df['at_bv'] = df['AVTOT'] / df['bldvol']\n",
    "\n",
    "## Build 45 Group by values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ninevars = ['fv_la','fv_ba','fv_bv','al_la','al_ba','al_bv','at_la','at_ba','at_bv']\n",
    "df = df.join(df.groupby(['ZIP'])[ninevars].mean(), on='ZIP', rsuffix='_zip')\n",
    "df = df.join(df.groupby(['ZIP3'])[ninevars].mean(), on='ZIP3', rsuffix='_zip3')\n",
    "df = df.join(df.groupby(['TAXCLASS'])[ninevars].mean(), on='TAXCLASS', rsuffix='_taxclass')\n",
    "df = df.join(df.groupby(['BORO'])[ninevars].mean(), on='BORO', rsuffix='_boro')\n",
    "# add stories bins\n",
    "df['story_bin'] = pd.qcut(df['STORIES'], 10, labels=False, duplicates = 'drop')\n",
    "df = df.join(df.groupby(['story_bin'])[ninevars].mean(), on='story_bin', rsuffix='_story')\n",
    "df = df.join(df.groupby(['BORO','TAXCLASS'])[ninevars].mean(), on=['BORO','TAXCLASS'], rsuffix='_BoroTax')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = df[['fv_la',\n",
    " 'fv_ba',\n",
    " 'fv_bv',\n",
    " 'al_la',\n",
    " 'al_ba',\n",
    " 'al_bv',\n",
    " 'at_la',\n",
    " 'at_ba',\n",
    " 'at_bv',\n",
    " 'fv_la_zip',\n",
    " 'fv_ba_zip',\n",
    " 'fv_bv_zip',\n",
    " 'al_la_zip',\n",
    " 'al_ba_zip',\n",
    " 'al_bv_zip',\n",
    " 'at_la_zip',\n",
    " 'at_ba_zip',\n",
    " 'at_bv_zip',\n",
    " 'fv_la_zip3',\n",
    " 'fv_ba_zip3',\n",
    " 'fv_bv_zip3',\n",
    " 'al_la_zip3',\n",
    " 'al_ba_zip3',\n",
    " 'al_bv_zip3',\n",
    " 'at_la_zip3',\n",
    " 'at_ba_zip3',\n",
    " 'at_bv_zip3',\n",
    " 'fv_la_taxclass',\n",
    " 'fv_ba_taxclass',\n",
    " 'fv_bv_taxclass',\n",
    " 'al_la_taxclass',\n",
    " 'al_ba_taxclass',\n",
    " 'al_bv_taxclass',\n",
    " 'at_la_taxclass',\n",
    " 'at_ba_taxclass',\n",
    " 'at_bv_taxclass',\n",
    " 'fv_la_boro',\n",
    " 'fv_ba_boro',\n",
    " 'fv_bv_boro',\n",
    " 'al_la_boro',\n",
    " 'al_ba_boro',\n",
    " 'al_bv_boro',\n",
    " 'at_la_boro',\n",
    " 'at_ba_boro',\n",
    " 'at_bv_boro',\n",
    " 'fv_la_story',\n",
    " 'fv_ba_story',\n",
    " 'fv_bv_story',\n",
    " 'al_la_story',\n",
    " 'al_ba_story',\n",
    " 'al_bv_story',\n",
    " 'at_la_story',\n",
    " 'at_ba_story',\n",
    " 'at_bv_story',\n",
    " 'fv_la_BoroTax',\n",
    " 'fv_ba_BoroTax',\n",
    " 'fv_bv_BoroTax',\n",
    " 'al_la_BoroTax',\n",
    " 'al_ba_BoroTax',\n",
    " 'al_bv_BoroTax',\n",
    " 'at_la_BoroTax',\n",
    " 'at_ba_BoroTax',\n",
    " 'at_bv_BoroTax']]\n",
    "\n",
    "# newdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Z scale to prepare for dimensionality reduction ----\n",
    "\n",
    "# Create the Scaler object\n",
    "scaler = preprocessing.StandardScaler()\n",
    "# Fit your data on the scaler object\n",
    "scaled_df = scaler.fit_transform(newdf)\n",
    "scaled_df = pd.DataFrame(scaled_df, columns=list(newdf))\n",
    "# scaled_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---- PCA: reduce dimensions ----\n",
    "pca = PCA(n_components = .85, svd_solver = 'full')\n",
    "pca.fit(scaled_df)\n",
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25435587 0.15936895 0.09257827 0.07293785 0.05871896 0.04988091\n",
      " 0.04670502 0.04424925 0.04028856 0.03322871]\n"
     ]
    }
   ],
   "source": [
    "# Percentage of variance explained by each of the selected components.\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformed dataframe\n",
    "pca_df = pd.DataFrame(data = pca.transform(scaled_df), columns =  [\"PC\" + str(i) for i in range(1, pca.n_components_+1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Z scale to PCA dataframe ----\n",
    "\n",
    "# Create the Scaler object\n",
    "scaler = preprocessing.StandardScaler()\n",
    "# Fit your data on the scaler object\n",
    "scaled_pca = scaler.fit_transform(pca_df)\n",
    "scaled_pca = pd.DataFrame(scaled_pca, columns=list(pca_df))\n",
    "# scaled_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Convert df to ndarray\n",
    "matrix_pca = scaled_pca.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  12873/1048575 [..............................] - ETA: 38:51 - loss: 0.4761"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-a383f9b0647c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mauto_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mmatrix_reproduce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimple_auto_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix_pca\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-23-a383f9b0647c>\u001b[0m in \u001b[0;36msimple_auto_encode\u001b[0;34m(input_matrix, compression_factor, encode_activation, decode_activation, optimizer, loss)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mauto_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mauto_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mauto_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mauto_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'`inputs` should be a list or tuple.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0mfeed_arrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m         \u001b[0marray_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_SESSION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m             \u001b[0mvariables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mcandidate_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_controller\u001b[0;34m(self, default)\u001b[0m\n\u001b[1;32m   5223\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_controller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5224\u001b[0m     context.context().context_switches.push(\n\u001b[0;32m-> 5225\u001b[0;31m         default.building_function, default.as_default)\n\u001b[0m\u001b[1;32m   5226\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5227\u001b[0m       with super(_DefaultGraphStack, self).get_controller(\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mpush\u001b[0;34m(self, is_building_function, enter_context_fn)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     self.stack.append(\n\u001b[0;32m--> 136\u001b[0;31m         ContextSwitch(is_building_function, enter_context_fn))\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(_cls, is_building_function, enter_context_fn)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Autoencoder\n",
    "def simple_auto_encode(input_matrix, compression_factor=2, encode_activation='tanh', decode_activation='linear',\n",
    "                       optimizer='Adadelta', loss='mean_squared_error'):\n",
    "    _, input_dim = matrix_pca.shape\n",
    "    encoding_dim = int(input_dim // compression_factor)\n",
    "\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    encode_layer = Dense(encoding_dim, activation=encode_activation)(input_layer)\n",
    "    decode_layer = Dense(input_dim, activation=decode_activation)(encode_layer)\n",
    "\n",
    "    auto_encoder = Model(input_layer, decode_layer)\n",
    "    auto_encoder.compile(optimizer=optimizer, loss=loss)\n",
    "    auto_encoder.fit(input_matrix, input_matrix, epochs=10, batch_size=1)\n",
    "\n",
    "    return auto_encoder.predict(input_matrix)\n",
    "\n",
    "matrix_reproduce = simple_auto_encode(matrix_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix_reproduce = np.loadtxt('data/reproduce.txt', dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = ((matrix_pca - matrix_reproduce)**2).mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(mse, bins=100, range=(0,0.001))\n",
    "plt.show()\n",
    "## Mse has the most ideal distribution. Therefore we use mse as fraud score 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurement = (((matrix_pca - matrix_reproduce)**2).sum(axis=1))**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(measurement, bins=100, range=(0,0.1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_pca[\"mae\"] = (matrix_pca - matrix_reproduce).mean(axis=1)\n",
    "scaled_pca[\"MAHAL\"]=(scaled_pca[\"PC1\"]**2+scaled_pca[\"PC2\"]**2+scaled_pca[\"PC3\"]**2+scaled_pca[\"PC4\"]**2+scaled_pca[\"PC5\"]**2+scaled_pca[\"PC6\"]**2+scaled_pca[\"PC7\"]**2+scaled_pca[\"PC8\"]**2+scaled_pca[\"PC9\"]**2+scaled_pca[\"PC10\"]**2)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(mae, bins=100, range=(0,0.1))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
