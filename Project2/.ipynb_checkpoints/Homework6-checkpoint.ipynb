{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pandas import ExcelWriter\n",
    "import random\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import ks_2samp\n",
    "import random\n",
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ddfec4caa280>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'AmountVariables.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "new_df = pd.read_csv('AmountVariables.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add a dummy variable as for sanity checks\n",
    "new_df['random'] = np.random.randint(0, 100, new_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate ks score for all variables\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "ks_df = pd.DataFrame()\n",
    "# ks_df['col'] = list(new_df)[10,:]\n",
    "ks_df['col'] = list(new_df[new_df.columns[~new_df.columns.isin(['Amount_cn_actual','Amount_mn_actual','Amount_cn_mn_actual','Amount_cn_zip_actual','Amount_cn_st_actual'])]])[10:]\n",
    "\n",
    "ks_df['p_value'] = ''\n",
    "ks_df['ks_score'] = ''\n",
    "for i in range(len(ks_df)):\n",
    "    ks_df['ks_score'][i] = ks_2samp(new_df[new_df['Fraud'] == 0][ks_df['col'][i]], new_df[new_df['Fraud'] == 1][ks_df['col'][i]])[0]\n",
    "    ks_df['p_value'][i] = ks_2samp(new_df[new_df['Fraud'] == 0][ks_df['col'][i]], new_df[new_df['Fraud'] == 1][ks_df['col'][i]])[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate fraud detection rate for all variables\n",
    "def fdr_cal(row):\n",
    "    col_name = row['col']\n",
    "    df_temp1 = new_df.sort_values(col_name, ascending = False)\n",
    "    df_temp_top1 = df_temp1[:round(len(new_df)*0.03)]\n",
    "    fdr1 = sum(df_temp_top1['Fraud'])/sum(df_temp1['Fraud'])\n",
    "    df_temp2 = new_df.sort_values(col_name, ascending = True)\n",
    "    df_temp_top2 = df_temp2[:round(len(new_df)*0.03)]\n",
    "    fdr2 = sum(df_temp_top2['Fraud'])/sum(df_temp2['Fraud'])\n",
    "    return max(fdr1, fdr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_df['fdr'] = ks_df.apply(lambda row: fdr_cal(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sort variables by fdr\n",
    "fdr_sorted_list = ks_df.sort_values('fdr', ascending = False)\n",
    "# fdr_sorted_list.to_pickle('fdr_sorted_list.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sort variables by ks_score\n",
    "ks_score_sorted_list = ks_df.sort_values('ks_score', ascending = False)\n",
    "# ks_score_sorted_list.to_pickle('ks_score_sorted_list.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ks_df['ks_rank'] = ks_df['ks_score'].rank(ascending = False)\n",
    "# ks_df['fdr_rank'] = ks_df['fdr'].rank(ascending = False)\n",
    "# ks_df['avg_rank'] = ks_df['ks_rank'] * 0.6 + ks_df['fdr_rank'] * 0.4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Divide into OOT and Train/Test\n",
    "\n",
    "## Standardizes in-place\n",
    "def standardize(df, colnames):\n",
    "    for col in colnames:\n",
    "        tmp_sd=df[col].std()\n",
    "        tmp_mean=df[col].mean()\n",
    "        df[col] = (df[col]-tmp_mean) / tmp_sd\n",
    "standardize(new_df,list(new_df)[11:])\n",
    "\n",
    "int_df = new_df[new_df[\"Date\"] < new_df[\"Date\"][84461]] \n",
    "oot_df = new_df[new_df[\"Date\"] > new_df[\"Date\"][84288]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate ks score for all variables\n",
    "def fdr_cal(row, df):\n",
    "    col_name = row['col']\n",
    "    df_temp = df.sort_values(col_name, ascending = False)\n",
    "    df_temp_top = df_temp[:round(len(df)*0.03)]\n",
    "    df_temp_bot = df_temp.tail(round(len(df)*0.03))\n",
    "    top_fdr= sum(df_temp_top['Fraud'])/sum(df_temp['Fraud'])\n",
    "    bot_fdr= sum(df_temp_bot['Fraud'])/sum(df_temp['Fraud'])\n",
    "#     print(str(top_fdr)+ \" \"+str(bot_fdr))\n",
    "    return(max(top_fdr, bot_fdr))\n",
    "\n",
    "def ks_fdr(df):\n",
    "    ks_df = pd.DataFrame()\n",
    "    ks_df['col'] = list(df)[10:]\n",
    "    ks_df['p_value'] = ''\n",
    "    ks_df['ks_score'] = ''\n",
    "    # Calc KS\n",
    "    for i in range(len(ks_df)):\n",
    "        ks_df['ks_score'][i] = ks_2samp(df[df['Fraud'] == 0][ks_df['col'][i]], df[df['Fraud'] == 1][ks_df['col'][i]])[0]\n",
    "        ks_df['p_value'][i] = ks_2samp(df[df['Fraud'] == 0][ks_df['col'][i]], df[df['Fraud'] == 1][ks_df['col'][i]])[1]\n",
    "    # Calc FDR\n",
    "    ks_df['fdr'] = ks_df.apply(lambda row: fdr_cal(row, df), axis=1)\n",
    "    return ks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ks_fdr_int = ks_fdr(int_df)\n",
    "ks_fdr_oot = ks_fdr(oot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_fdr_int['ks_rank'] = ks_fdr_int['ks_score'].rank(ascending = False)\n",
    "ks_fdr_int['fdr_rank'] = ks_fdr_int['fdr'].rank(ascending = False)\n",
    "ks_fdr_int['avg_rank'] = ks_fdr_int['ks_rank'] * 0.6 + ks_fdr_int['fdr_rank'] * 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = ks_fdr_int.sort_values(\"avg_rank\")[\"col\"].head(80)\n",
    "top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "estimator = LogisticRegression(solver=\"liblinear\", max_iter=300)\n",
    "selector = RFE(estimator, 20, step=1)\n",
    "selector = selector.fit(int_df[top_features], int_df[\"Fraud\"])\n",
    "final_features=top_features[selector.support_]\n",
    "selected_df = pd.DataFrame(int_df, columns=list(final_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "selected_df.to_csv(\"selected_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_df = pd.read_csv('selected_df.csv')\n",
    "selected_df = selected_df.drop(columns=['Unnamed: 0', 'Unnamed: 0.1'])\n",
    "##train, test = train_test_split(selected_df, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(selected_df)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = selected_df.loc[:, selected_df.columns != 'Fraud']\n",
    "y = selected_df.loc[:, selected_df.columns == 'Fraud']\n",
    "from imblearn.over_sampling import SMOTE\n",
    "os = SMOTE(random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "columns = X_train.columns\n",
    "os_data_X,os_data_y=os.fit_sample(X_train, y_train)\n",
    "os_data_X = pd.DataFrame(data=os_data_X,columns=columns )\n",
    "os_data_y= pd.DataFrame(data=os_data_y,columns=['Fraud'])\n",
    "# we can Check the numbers of our data\n",
    "print(\"length of oversampled data is \",len(os_data_X))\n",
    "print(\"Number of no subscription in oversampled data\",len(os_data_y[os_data_y['Fraud']==0]))\n",
    "print(\"Number of subscription\",len(os_data_y[os_data_y['Fraud']==1]))\n",
    "print(\"Proportion of no subscription data in oversampled data is \",len(os_data_y[os_data_y['Fraud']==0])/len(os_data_X))\n",
    "print(\"Proportion of subscription data in oversampled data is \",len(os_data_y[os_data_y['Fraud']==1])/len(os_data_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "logit_model=sm.Logit(os_data_y,os_data_X)\n",
    "result=logit_model.fit()\n",
    "print(result.summary2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['Amount_cn_st_actual',\n",
    " 'Amount_mn_30d_act-avg',\n",
    " 'Amount_cn_14d_act-avg',\n",
    " 'Amount_cn_7d_act-avg',\n",
    " 'Amount_cn_st_7d_sum',\n",
    " 'Amount_cn_st_4d_avg',\n",
    " 'Amount_cn_14d_sum',\n",
    " 'Amount_cn_st_30d_act-avg',\n",
    " 'Amount_cn_st_7d_avg',\n",
    " 'Amount_mn_30d_act/avg',\n",
    " 'Amount_cn_30d_act/avg',\n",
    " 'Amount_cn_mn_7d_sum',\n",
    " 'Amount_cn_30d_sum',\n",
    " 'Amount_cn_30d_act/sum',\n",
    " 'Amount_cn_zip_30d_avg',\n",
    " 'Amount_cn_zip_30d_sum',\n",
    " 'Amount_cn_st_7d_act-avg',\n",
    " 'Amount_mn_1d_avg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=os_data_X[selected_features]\n",
    "y=os_data_y['Fraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_model=sm.Logit(y,X)\n",
    "result=logit_model.fit()\n",
    "print(result.summary2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logreg.predict(X_test[selected_features])\n",
    "sum(y_pred == y_test['Fraud'])/len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test[selected_features], y_test['Fraud'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = oot_df.loc[:, selected_features]\n",
    "y = oot_df.loc[:, oot_df.columns == 'Fraud']\n",
    "y_pred = logreg.predict(X[selected_features])\n",
    "sum(y_pred == oot_df['Fraud'])/len(oot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "logit_roc_auc = roc_auc_score(y_test, logreg.predict(X_test[selected_features]))\n",
    "fpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test[selected_features])[:,1])\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('Log_ROC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_roc_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "results without oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X_train[selected_features]\n",
    "y=y_train['Fraud']\n",
    "logreg2 = LogisticRegression()\n",
    "logreg2.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logreg2.predict(X_test[selected_features])\n",
    "sum(y_pred == y_test['Fraud'])/len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "logit_roc_auc = roc_auc_score(y_test, logreg2.predict(X_test[selected_features]))\n",
    "fpr, tpr, thresholds = roc_curve(y_test, logreg2.predict_proba(X_test[selected_features])[:,1])\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "##plt.savefig('Log_ROC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=os_data_X[features]\n",
    "y=os_data_y['Fraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(13,13,13),max_iter=500)\n",
    "mlp.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = mlp.predict(X_test[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "35*1000 + 361*20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "logit_roc_auc = roc_auc_score(y_test, predictions)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, mlp.predict_proba(X_test[features])[:,1])\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "##plt.savefig('Log_ROC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.predict_proba(X_test[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = oot_df.loc[:, features]\n",
    "y = oot_df.loc[:, oot_df.columns == 'Fraud']\n",
    "y_pred = mlp.predict(X[features])\n",
    "sum(y_pred == oot_df['Fraud'])/len(oot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(confusion_matrix(y,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "logit_roc_auc = roc_auc_score(y, y_pred)\n",
    "fpr, tpr, thresholds = roc_curve(y, mlp.predict_proba(X[features])[:,1])\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "##plt.savefig('Log_ROC')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
