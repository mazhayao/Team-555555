{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pandas import ExcelWriter\n",
    "import random\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import ks_2samp\n",
    "import random\n",
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.6 s, sys: 230 ms, total: 14.8 s\n",
      "Wall time: 15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = pd.read_excel('card transactions.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th># NaN</th>\n",
       "      <th>% populated</th>\n",
       "      <th># unique</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Recnum</td>\n",
       "      <td>0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>96753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cardnum</td>\n",
       "      <td>0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Date</td>\n",
       "      <td>0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Merchnum</td>\n",
       "      <td>3375</td>\n",
       "      <td>96.511736</td>\n",
       "      <td>13091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Merch description</td>\n",
       "      <td>0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>13126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Merch state</td>\n",
       "      <td>1195</td>\n",
       "      <td>98.764896</td>\n",
       "      <td>227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Merch zip</td>\n",
       "      <td>4656</td>\n",
       "      <td>95.187746</td>\n",
       "      <td>4567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Transtype</td>\n",
       "      <td>0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Amount</td>\n",
       "      <td>0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>34909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Fraud</td>\n",
       "      <td>0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            variable  # NaN  % populated # unique\n",
       "0             Recnum      0   100.000000    96753\n",
       "1            Cardnum      0   100.000000     1645\n",
       "2               Date      0   100.000000      365\n",
       "3           Merchnum   3375    96.511736    13091\n",
       "4  Merch description      0   100.000000    13126\n",
       "5        Merch state   1195    98.764896      227\n",
       "6          Merch zip   4656    95.187746     4567\n",
       "7          Transtype      0   100.000000        4\n",
       "8             Amount      0   100.000000    34909\n",
       "9              Fraud      0   100.000000        2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summary\n",
    "numrecords = len(df)\n",
    "stat_df = pd.DataFrame(df.isnull().sum(axis = 0))\n",
    "stat_df = stat_df.reset_index().rename(columns = {'index':'variable', 0 :\"# NaN\"})\n",
    "stat_df[\"% populated\"] = (1 - stat_df[\"# NaN\"]/numrecords)*100\n",
    "stat_df[\"# unique\"] = ''\n",
    "\n",
    "for i in range(len(list(df))):\n",
    "    stat_df[\"# unique\"][i] = df[list(df)[i]].nunique()\n",
    "    \n",
    "stat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stat_df.to_excel('stat_df.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96397"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Identify any exclusions, bad records\n",
    "\n",
    "# Remove single large transaction outlier in Amount\n",
    "df = df[df['Amount']<3000000]\n",
    "\n",
    "# Remove all but P transaction type\n",
    "df = df[df['Transtype'] == 'P']\n",
    "\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1414"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------------- Fill in NAs ----------\n",
    "\n",
    "### Merchnum \n",
    "# Transform 0 values into NA\n",
    "df.loc[df['Merchnum'] == '0', 'Merchnum'] = np.NaN\n",
    "\n",
    "# fill most frequenct merchnum using merch description\n",
    "dict1 = df.set_index('Merch description')['Merchnum'].to_dict()\n",
    "df['Merchnum'].fillna(df['Merch description'].map(dict1), inplace = True)\n",
    "\n",
    "# fill most frequenct merchnum in this zip\n",
    "dict1 = dict(zip(df['Merch zip'].dropna(), df['Merchnum']))\n",
    "df['Merchnum'].fillna(df['Merch zip'].map(dict1), inplace = True)\n",
    "\n",
    "pd.isnull(df['Merchnum']).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3810"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Merch state and zip\n",
    "numrecords - (df['Merch state'].isna() == df['Merch zip'].isna()).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "543"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Merch zip\n",
    "# df.groupby(['Merch description','Merch zip']).size()\n",
    "\n",
    "# fill NA in Merch zip using Merch description\n",
    "dict1 = df.set_index('Merch description')['Merch zip'].to_dict()\n",
    "df['Merch zip'].fillna(df['Merch description'].map(dict1), inplace = True)\n",
    "\n",
    "# fill NA in Merch zip using Merchnum\n",
    "dict1 = df.set_index('Merchnum')['Merch zip'].to_dict()\n",
    "df['Merch zip'].fillna(df['Merchnum'].map(dict1), inplace = True)\n",
    "\n",
    "# fill NA in Merch zip using Cardnum\n",
    "dict1 = df.set_index('Cardnum')['Merch zip'].to_dict()\n",
    "df['Merch zip'].fillna(df['Cardnum'].map(dict1), inplace = True)\n",
    "# It will automatically choose zip with highest frequency\n",
    "\n",
    "pd.isnull(df['Merch zip']).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60108.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic = dict(zip(df['Cardnum'], df['Merch zip']))\n",
    "dic[5142310525]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Merch state\n",
    "# df.groupby(['Merch description','Merch state']).size()\n",
    "# Each Merch description has one matched state\n",
    "\n",
    "# fill NA in Merch state using Merch description\n",
    "dict1 = df.set_index('Merch description')['Merch state'].to_dict()\n",
    "df['Merch state'].fillna(df['Merch description'].map(dict1), inplace = True)\n",
    "\n",
    "# fill NA in Merch state using Merchnum\n",
    "dict1 = df.set_index('Merchnum')['Merch state'].to_dict()\n",
    "df['Merch state'].fillna(df['Merchnum'].map(dict1), inplace = True)\n",
    "\n",
    "# fill NA in Merch state using Merch zip\n",
    "\n",
    "dict1 = dict(zip(df['Merch zip'].dropna(), df['Merch state']))\n",
    "df['Merch state'].fillna(df['Merch zip'].map(dict1), inplace = True)\n",
    "\n",
    "# fill NA in Merch state using Cardnum\n",
    "dict1 = df.set_index('Cardnum')['Merch state'].to_dict()\n",
    "df['Merch state'].fillna(df['Cardnum'].map(dict1), inplace = True)\n",
    "\n",
    "pd.isnull(df['Merch state']).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94631"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop remained NA in three variables\n",
    "df = df.dropna()\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.groupby(['Cardnum','Date'])['Amount'].value_counts()\n",
    "# df[df['Cardnum'] == 5142190439]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# --------------- Create variables ----------\n",
    "### Amount expert variables\n",
    "\n",
    "# Cardnum\n",
    "# sum the amount at the same day first and then rolling\n",
    "df2 = df.groupby(['Cardnum','Date'])['Amount'].sum().to_frame().reset_index().set_index('Date')\n",
    "new_df = df.join(df.groupby(['Cardnum','Date'])['Amount'].sum(),on = ['Cardnum','Date'], rsuffix = '_cn_actual')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (closed = 'left') in rolling function means when calculating past n days we exlude today's transactions\n",
    "\n",
    "def amount_mean_func(column, window, rsuffix):\n",
    "    dataframe = pd.DataFrame()\n",
    "    dataframe = new_df.join(df2.groupby([column])['Amount'].rolling(window = window, closed = 'left').mean(), on = [column,'Date'], rsuffix = rsuffix)\n",
    "    return dataframe\n",
    "\n",
    "# def amount_max_func(column, window, rsuffix):\n",
    "#     dataframe = pd.DataFrame()\n",
    "#     dataframe = new_df.join(df2.groupby([column])['Amount'].rolling(window = window).max(), on = [column,'Date'], rsuffix = rsuffix)\n",
    "#     return dataframe\n",
    "\n",
    "# def amount_median_func(column, window, rsuffix):\n",
    "#     dataframe = pd.DataFrame()\n",
    "#     dataframe = new_df.join(df2.groupby([column])['Amount'].rolling(window = window).median(), on = [column,'Date'], rsuffix = rsuffix)\n",
    "#     return dataframe\n",
    "\n",
    "def amount_sum_func(column, window, rsuffix):\n",
    "    dataframe = pd.DataFrame()\n",
    "    dataframe = new_df.join(df2.groupby([column])['Amount'].rolling(window = window, closed = 'left').sum(), on = [column,'Date'], rsuffix = rsuffix)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.groupby(['Cardnum','Date'])['Amount'].sum().to_frame().reset_index().set_index('Date')\n",
    "new_df = df.join(df.groupby(['Cardnum','Date'])['Amount'].sum(),on = ['Cardnum','Date'], rsuffix = '_cn_actual')\n",
    "\n",
    "new_df = amount_mean_func(column = 'Cardnum',window = '1D',rsuffix = '_cn_1d_avg')\n",
    "# new_df = amount_max_func(column = 'Cardnum',window = '1D',rsuffix = '_cn_1d_max')\n",
    "# new_df = amount_median_func(column = 'Cardnum',window = '1D',rsuffix = '_cn_1d_median')\n",
    "new_df = amount_sum_func(column = 'Cardnum',window = '1D',rsuffix = '_cn_1d_sum')\n",
    "new_df['Amount_cn_1d_act-avg'] = new_df['Amount_cn_actual'] - new_df['Amount_cn_1d_avg']\n",
    "# new_df['Amount_cn_1d_act-median'] = new_df['Amount_cn_actual'] - new_df['Amount_cn_1d_median']\n",
    "new_df['Amount_cn_1d_act/avg'] = new_df['Amount_cn_actual']/new_df['Amount_cn_1d_avg']\n",
    "# new_df['Amount_cn_1d_act/max'] = new_df['Amount_cn_actual']/new_df['Amount_cn_1d_max']\n",
    "new_df['Amount_cn_1d_act/sum'] = new_df['Amount_cn_actual']/new_df['Amount_cn_1d_sum']\n",
    "# new_df['Amount_cn_1d_act/median'] = new_df['Amount_cn_actual']/new_df['Amount_cn_1d_median']\n",
    "\n",
    "new_df = amount_mean_func(column = 'Cardnum',window = '4D',rsuffix = '_cn_4d_avg')\n",
    "new_df = amount_sum_func(column = 'Cardnum',window = '4D',rsuffix = '_cn_4d_sum')\n",
    "new_df['Amount_cn_4d_act-avg'] = new_df['Amount_cn_actual'] - new_df['Amount_cn_4d_avg']\n",
    "new_df['Amount_cn_4d_act/avg'] = new_df['Amount_cn_actual']/new_df['Amount_cn_4d_avg']\n",
    "new_df['Amount_cn_4d_act/sum'] = new_df['Amount_cn_actual']/new_df['Amount_cn_4d_sum']\n",
    "\n",
    "new_df = amount_mean_func(column = 'Cardnum',window = '7D',rsuffix = '_cn_7d_avg')\n",
    "new_df = amount_sum_func(column = 'Cardnum',window = '7D',rsuffix = '_cn_7d_sum')\n",
    "new_df['Amount_cn_7d_act-avg'] = new_df['Amount_cn_actual'] - new_df['Amount_cn_7d_avg']\n",
    "new_df['Amount_cn_7d_act/avg'] = new_df['Amount_cn_actual']/new_df['Amount_cn_7d_avg']\n",
    "new_df['Amount_cn_7d_act/sum'] = new_df['Amount_cn_actual']/new_df['Amount_cn_7d_sum']\n",
    "\n",
    "new_df = amount_mean_func(column = 'Cardnum',window = '14D',rsuffix = '_cn_14d_avg')\n",
    "new_df = amount_sum_func(column = 'Cardnum',window = '14D',rsuffix = '_cn_14d_sum')\n",
    "new_df['Amount_cn_14d_act-avg'] = new_df['Amount_cn_actual'] - new_df['Amount_cn_14d_avg']\n",
    "new_df['Amount_cn_14d_act/avg'] = new_df['Amount_cn_actual']/new_df['Amount_cn_14d_avg']\n",
    "new_df['Amount_cn_14d_act/sum'] = new_df['Amount_cn_actual']/new_df['Amount_cn_14d_sum']\n",
    "\n",
    "new_df = amount_mean_func(column = 'Cardnum',window = '30D',rsuffix = '_cn_30d_avg')\n",
    "new_df = amount_sum_func(column = 'Cardnum',window = '30D',rsuffix = '_cn_30d_sum')\n",
    "new_df['Amount_cn_30d_act-avg'] = new_df['Amount_cn_actual'] - new_df['Amount_cn_30d_avg']\n",
    "new_df['Amount_cn_30d_act/avg'] = new_df['Amount_cn_actual']/new_df['Amount_cn_30d_avg']\n",
    "new_df['Amount_cn_30d_act/sum'] = new_df['Amount_cn_actual']/new_df['Amount_cn_30d_sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.groupby(['Merchnum','Date'])['Amount'].sum().to_frame().reset_index().set_index('Date')\n",
    "new_df = new_df.join(df.groupby(['Merchnum','Date'])['Amount'].sum(),on = ['Merchnum','Date'], rsuffix = '_mn_actual')\n",
    "\n",
    "new_df = amount_mean_func(column = 'Merchnum',window = '1D',rsuffix = '_mn_1d_avg')\n",
    "new_df = amount_sum_func(column = 'Merchnum',window = '1D',rsuffix = '_mn_1d_sum')\n",
    "new_df['Amount_mn_1d_act-avg'] = new_df['Amount_mn_actual'] - new_df['Amount_mn_1d_avg']\n",
    "new_df['Amount_mn_1d_act/avg'] = new_df['Amount_mn_actual']/new_df['Amount_mn_1d_avg']\n",
    "new_df['Amount_mn_1d_act/sum'] = new_df['Amount_mn_actual']/new_df['Amount_mn_1d_sum']\n",
    "\n",
    "new_df = amount_mean_func(column = 'Merchnum',window = '4D',rsuffix = '_mn_4d_avg')\n",
    "new_df = amount_sum_func(column = 'Merchnum',window = '4D',rsuffix = '_mn_4d_sum')\n",
    "new_df['Amount_mn_4d_act-avg'] = new_df['Amount_mn_actual'] - new_df['Amount_mn_4d_avg']\n",
    "new_df['Amount_mn_4d_act/avg'] = new_df['Amount_mn_actual']/new_df['Amount_mn_4d_avg']\n",
    "new_df['Amount_mn_4d_act/sum'] = new_df['Amount_mn_actual']/new_df['Amount_mn_4d_sum']\n",
    "\n",
    "new_df = amount_mean_func(column = 'Merchnum',window = '7D',rsuffix = '_mn_7d_avg')\n",
    "new_df = amount_sum_func(column = 'Merchnum',window = '7D',rsuffix = '_mn_7d_sum')\n",
    "new_df['Amount_mn_7d_act-avg'] = new_df['Amount_mn_actual'] - new_df['Amount_mn_7d_avg']\n",
    "new_df['Amount_mn_7d_act/avg'] = new_df['Amount_mn_actual']/new_df['Amount_mn_7d_avg']\n",
    "new_df['Amount_mn_7d_act/sum'] = new_df['Amount_mn_actual']/new_df['Amount_mn_7d_sum']\n",
    "\n",
    "new_df = amount_mean_func(column = 'Merchnum',window = '14D',rsuffix = '_mn_14d_avg')\n",
    "new_df = amount_sum_func(column = 'Merchnum',window = '14D',rsuffix = '_mn_14d_sum')\n",
    "new_df['Amount_mn_14d_act-avg'] = new_df['Amount_mn_actual'] - new_df['Amount_mn_14d_avg']\n",
    "new_df['Amount_mn_14d_act/avg'] = new_df['Amount_mn_actual']/new_df['Amount_mn_14d_avg']\n",
    "new_df['Amount_mn_14d_act/sum'] = new_df['Amount_mn_actual']/new_df['Amount_mn_14d_sum']\n",
    "\n",
    "new_df = amount_mean_func(column = 'Merchnum',window = '30D',rsuffix = '_mn_30d_avg')\n",
    "new_df = amount_sum_func(column = 'Merchnum',window = '30D',rsuffix = '_mn_30d_sum')\n",
    "new_df['Amount_mn_30d_act-avg'] = new_df['Amount_mn_actual'] - new_df['Amount_mn_30d_avg']\n",
    "new_df['Amount_mn_30d_act/avg'] = new_df['Amount_mn_actual']/new_df['Amount_mn_30d_avg']\n",
    "new_df['Amount_mn_30d_act/sum'] = new_df['Amount_mn_actual']/new_df['Amount_mn_30d_sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.groupby(['Cardnum','Merchnum','Date'])['Amount'].sum().to_frame().reset_index().set_index('Date')\n",
    "new_df = new_df.join(df.groupby(['Cardnum','Merchnum','Date'])['Amount'].sum(),on = ['Cardnum','Merchnum','Date'], rsuffix = '_cn_mn_actual')\n",
    "\n",
    "new_df = new_df.join(df2.groupby(['Cardnum','Merchnum'])['Amount'].rolling(window = '1D', closed = 'left').mean(), on = ['Cardnum','Merchnum','Date'], rsuffix = '_cn_mn_1d_avg')\n",
    "new_df = new_df.join(df2.groupby(['Cardnum','Merchnum'])['Amount'].rolling(window = '1D', closed = 'left').sum(), on = ['Cardnum','Merchnum','Date'], rsuffix = '_cn_mn_1d_sum')\n",
    "new_df['Amount_cn_mn_1d_act-avg'] = new_df['Amount_cn_mn_actual'] - new_df['Amount_cn_mn_1d_avg']\n",
    "new_df['Amount_cn_mn_1d_act/avg'] = new_df['Amount_cn_mn_actual']/new_df['Amount_cn_mn_1d_avg']\n",
    "new_df['Amount_cn_mn_1d_act/sum'] = new_df['Amount_cn_mn_actual']/new_df['Amount_cn_mn_1d_sum']\n",
    "\n",
    "new_df = new_df.join(df2.groupby(['Cardnum','Merchnum'])['Amount'].rolling(window = '4D', closed = 'left').mean(), on = ['Cardnum','Merchnum','Date'], rsuffix = '_cn_mn_4d_avg')\n",
    "new_df = new_df.join(df2.groupby(['Cardnum','Merchnum'])['Amount'].rolling(window = '4D', closed = 'left').sum(), on = ['Cardnum','Merchnum','Date'], rsuffix = '_cn_mn_4d_sum')\n",
    "new_df['Amount_cn_mn_4d_act-avg'] = new_df['Amount_cn_mn_actual'] - new_df['Amount_cn_mn_4d_avg']\n",
    "new_df['Amount_cn_mn_4d_act/avg'] = new_df['Amount_cn_mn_actual']/new_df['Amount_cn_mn_4d_avg']\n",
    "new_df['Amount_cn_mn_4d_act/sum'] = new_df['Amount_cn_mn_actual']/new_df['Amount_cn_mn_4d_sum']\n",
    "\n",
    "new_df = new_df.join(df2.groupby(['Cardnum','Merchnum'])['Amount'].rolling(window = '7D', closed = 'left').mean(), on = ['Cardnum','Merchnum','Date'], rsuffix = '_cn_mn_7d_avg')\n",
    "new_df = new_df.join(df2.groupby(['Cardnum','Merchnum'])['Amount'].rolling(window = '7D', closed = 'left').sum(), on = ['Cardnum','Merchnum','Date'], rsuffix = '_cn_mn_7d_sum')\n",
    "new_df['Amount_cn_mn_7d_act-avg'] = new_df['Amount_cn_mn_actual'] - new_df['Amount_cn_mn_7d_avg']\n",
    "new_df['Amount_cn_mn_7d_act/avg'] = new_df['Amount_cn_mn_actual']/new_df['Amount_cn_mn_7d_avg']\n",
    "new_df['Amount_cn_mn_7d_act/sum'] = new_df['Amount_cn_mn_actual']/new_df['Amount_cn_mn_7d_sum']\n",
    "\n",
    "new_df = new_df.join(df2.groupby(['Cardnum','Merchnum'])['Amount'].rolling(window = '14D', closed = 'left').mean(), on = ['Cardnum','Merchnum','Date'], rsuffix = '_cn_mn_14d_avg')\n",
    "new_df = new_df.join(df2.groupby(['Cardnum','Merchnum'])['Amount'].rolling(window = '14D', closed = 'left').sum(), on = ['Cardnum','Merchnum','Date'], rsuffix = '_cn_mn_14d_sum')\n",
    "new_df['Amount_cn_mn_14d_act-avg'] = new_df['Amount_cn_mn_actual'] - new_df['Amount_cn_mn_14d_avg']\n",
    "new_df['Amount_cn_mn_14d_act/avg'] = new_df['Amount_cn_mn_actual']/new_df['Amount_cn_mn_14d_avg']\n",
    "new_df['Amount_cn_mn_14d_act/sum'] = new_df['Amount_cn_mn_actual']/new_df['Amount_cn_mn_14d_sum']\n",
    "\n",
    "new_df = new_df.join(df2.groupby(['Cardnum','Merchnum'])['Amount'].rolling(window = '30D', closed = 'left').mean(), on = ['Cardnum','Merchnum','Date'], rsuffix = '_cn_mn_30d_avg')\n",
    "new_df = new_df.join(df2.groupby(['Cardnum','Merchnum'])['Amount'].rolling(window = '30D', closed = 'left').sum(), on = ['Cardnum','Merchnum','Date'], rsuffix = '_cn_mn_30d_sum')\n",
    "new_df['Amount_cn_mn_30d_act-avg'] = new_df['Amount_cn_mn_actual'] - new_df['Amount_cn_mn_30d_avg']\n",
    "new_df['Amount_cn_mn_30d_act/avg'] = new_df['Amount_cn_mn_actual']/new_df['Amount_cn_mn_30d_avg']\n",
    "new_df['Amount_cn_mn_30d_act/sum'] = new_df['Amount_cn_mn_actual']/new_df['Amount_cn_mn_30d_sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.groupby(['Cardnum','Merch zip','Date'])['Amount'].sum().to_frame().reset_index().set_index('Date')\n",
    "new_df = new_df.join(df.groupby(['Cardnum','Merch zip','Date'])['Amount'].sum(),on = ['Cardnum','Merch zip','Date'], rsuffix = '_cn_zip_actual')\n",
    "\n",
    "new_df = new_df.join(df2.groupby(['Cardnum','Merch zip'])['Amount'].rolling(window = '1D', closed = 'left').mean(), on = ['Cardnum','Merch zip','Date'], rsuffix = '_cn_zip_1d_avg')\n",
    "new_df = new_df.join(df2.groupby(['Cardnum','Merch zip'])['Amount'].rolling(window = '1D', closed = 'left').sum(), on = ['Cardnum','Merch zip','Date'], rsuffix = '_cn_zip_1d_sum')\n",
    "new_df['Amount_cn_zip_1d_act-avg'] = new_df['Amount_cn_zip_actual'] - new_df['Amount_cn_zip_1d_avg']\n",
    "new_df['Amount_cn_zip_1d_act/avg'] = new_df['Amount_cn_zip_actual']/new_df['Amount_cn_zip_1d_avg']\n",
    "new_df['Amount_cn_zip_1d_act/sum'] = new_df['Amount_cn_zip_actual']/new_df['Amount_cn_zip_1d_sum']\n",
    "\n",
    "new_df = new_df.join(df2.groupby(['Cardnum','Merch zip'])['Amount'].rolling(window = '4D', closed = 'left').mean(), on = ['Cardnum','Merch zip','Date'], rsuffix = '_cn_zip_4d_avg')\n",
    "new_df = new_df.join(df2.groupby(['Cardnum','Merch zip'])['Amount'].rolling(window = '4D', closed = 'left').sum(), on = ['Cardnum','Merch zip','Date'], rsuffix = '_cn_zip_4d_sum')\n",
    "new_df['Amount_cn_zip_4d_act-avg'] = new_df['Amount_cn_zip_actual'] - new_df['Amount_cn_zip_4d_avg']\n",
    "new_df['Amount_cn_zip_4d_act/avg'] = new_df['Amount_cn_zip_actual']/new_df['Amount_cn_zip_4d_avg']\n",
    "new_df['Amount_cn_zip_4d_act/sum'] = new_df['Amount_cn_zip_actual']/new_df['Amount_cn_zip_4d_sum']\n",
    "\n",
    "new_df = new_df.join(df2.groupby(['Cardnum','Merch zip'])['Amount'].rolling(window = '7D', closed = 'left').mean(), on = ['Cardnum','Merch zip','Date'], rsuffix = '_cn_zip_7d_avg')\n",
    "new_df = new_df.join(df2.groupby(['Cardnum','Merch zip'])['Amount'].rolling(window = '7D', closed = 'left').sum(), on = ['Cardnum','Merch zip','Date'], rsuffix = '_cn_zip_7d_sum')\n",
    "new_df['Amount_cn_zip_7d_act-avg'] = new_df['Amount_cn_zip_actual'] - new_df['Amount_cn_zip_7d_avg']\n",
    "new_df['Amount_cn_zip_7d_act/avg'] = new_df['Amount_cn_zip_actual']/new_df['Amount_cn_zip_7d_avg']\n",
    "new_df['Amount_cn_zip_7d_act/sum'] = new_df['Amount_cn_zip_actual']/new_df['Amount_cn_zip_7d_sum']\n",
    "\n",
    "new_df = new_df.join(df2.groupby(['Cardnum','Merch zip'])['Amount'].rolling(window = '14D', closed = 'left').mean(), on = ['Cardnum','Merch zip','Date'], rsuffix = '_cn_zip_14d_avg')\n",
    "new_df = new_df.join(df2.groupby(['Cardnum','Merch zip'])['Amount'].rolling(window = '14D', closed = 'left').sum(), on = ['Cardnum','Merch zip','Date'], rsuffix = '_cn_zip_14d_sum')\n",
    "new_df['Amount_cn_zip_14d_act-avg'] = new_df['Amount_cn_zip_actual'] - new_df['Amount_cn_zip_14d_avg']\n",
    "new_df['Amount_cn_zip_14d_act/avg'] = new_df['Amount_cn_zip_actual']/new_df['Amount_cn_zip_14d_avg']\n",
    "new_df['Amount_cn_zip_14d_act/sum'] = new_df['Amount_cn_zip_actual']/new_df['Amount_cn_zip_14d_sum']\n",
    "\n",
    "new_df = new_df.join(df2.groupby(['Cardnum','Merch zip'])['Amount'].rolling(window = '30D', closed = 'left').mean(), on = ['Cardnum','Merch zip','Date'], rsuffix = '_cn_zip_30d_avg')\n",
    "new_df = new_df.join(df2.groupby(['Cardnum','Merch zip'])['Amount'].rolling(window = '30D', closed = 'left').sum(), on = ['Cardnum','Merch zip','Date'], rsuffix = '_cn_zip_30d_sum')\n",
    "new_df['Amount_cn_zip_30d_act-avg'] = new_df['Amount_cn_zip_actual'] - new_df['Amount_cn_zip_30d_avg']\n",
    "new_df['Amount_cn_zip_30d_act/avg'] = new_df['Amount_cn_zip_actual']/new_df['Amount_cn_zip_30d_avg']\n",
    "new_df['Amount_cn_zip_30d_act/sum'] = new_df['Amount_cn_zip_actual']/new_df['Amount_cn_zip_30d_sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.groupby(['Cardnum','Merch state','Date'])['Amount'].sum().to_frame().reset_index().set_index('Date')\n",
    "new_df = new_df.join(df.groupby(['Cardnum','Merch state','Date'])['Amount'].sum(),on = ['Cardnum','Merch state','Date'], rsuffix = '_cn_st_actual')\n",
    "\n",
    "new_df = new_df.join(df2.groupby(['Cardnum','Merch state'])['Amount'].rolling(window = '1D', closed = 'left').mean(), on = ['Cardnum','Merch state','Date'], rsuffix = '_cn_st_1d_avg')\n",
    "new_df = new_df.join(df2.groupby(['Cardnum','Merch state'])['Amount'].rolling(window = '1D', closed = 'left').sum(), on = ['Cardnum','Merch state','Date'], rsuffix = '_cn_st_1d_sum')\n",
    "new_df['Amount_cn_st_1d_act-avg'] = new_df['Amount_cn_st_actual'] - new_df['Amount_cn_st_1d_avg']\n",
    "new_df['Amount_cn_st_1d_act/avg'] = new_df['Amount_cn_st_actual']/new_df['Amount_cn_st_1d_avg']\n",
    "new_df['Amount_cn_st_1d_act/sum'] = new_df['Amount_cn_st_actual']/new_df['Amount_cn_st_1d_sum']\n",
    "\n",
    "new_df = new_df.join(df2.groupby(['Cardnum','Merch state'])['Amount'].rolling(window = '4D', closed = 'left').mean(), on = ['Cardnum','Merch state','Date'], rsuffix = '_cn_st_4d_avg')\n",
    "new_df = new_df.join(df2.groupby(['Cardnum','Merch state'])['Amount'].rolling(window = '4D', closed = 'left').sum(), on = ['Cardnum','Merch state','Date'], rsuffix = '_cn_st_4d_sum')\n",
    "new_df['Amount_cn_st_4d_act-avg'] = new_df['Amount_cn_st_actual'] - new_df['Amount_cn_st_4d_avg']\n",
    "new_df['Amount_cn_st_4d_act/avg'] = new_df['Amount_cn_st_actual']/new_df['Amount_cn_st_4d_avg']\n",
    "new_df['Amount_cn_st_4d_act/sum'] = new_df['Amount_cn_st_actual']/new_df['Amount_cn_st_4d_sum']\n",
    "\n",
    "new_df = new_df.join(df2.groupby(['Cardnum','Merch state'])['Amount'].rolling(window = '7D', closed = 'left').mean(), on = ['Cardnum','Merch state','Date'], rsuffix = '_cn_st_7d_avg')\n",
    "new_df = new_df.join(df2.groupby(['Cardnum','Merch state'])['Amount'].rolling(window = '7D', closed = 'left').sum(), on = ['Cardnum','Merch state','Date'], rsuffix = '_cn_st_7d_sum')\n",
    "new_df['Amount_cn_st_7d_act-avg'] = new_df['Amount_cn_st_actual'] - new_df['Amount_cn_st_7d_avg']\n",
    "new_df['Amount_cn_st_7d_act/avg'] = new_df['Amount_cn_st_actual']/new_df['Amount_cn_st_7d_avg']\n",
    "new_df['Amount_cn_st_7d_act/sum'] = new_df['Amount_cn_st_actual']/new_df['Amount_cn_st_7d_sum']\n",
    "\n",
    "new_df = new_df.join(df2.groupby(['Cardnum','Merch state'])['Amount'].rolling(window = '14D', closed = 'left').mean(), on = ['Cardnum','Merch state','Date'], rsuffix = '_cn_st_14d_avg')\n",
    "new_df = new_df.join(df2.groupby(['Cardnum','Merch state'])['Amount'].rolling(window = '14D', closed = 'left').sum(), on = ['Cardnum','Merch state','Date'], rsuffix = '_cn_st_14d_sum')\n",
    "new_df['Amount_cn_st_14d_act-avg'] = new_df['Amount_cn_st_actual'] - new_df['Amount_cn_st_14d_avg']\n",
    "new_df['Amount_cn_st_14d_act/avg'] = new_df['Amount_cn_st_actual']/new_df['Amount_cn_st_14d_avg']\n",
    "new_df['Amount_cn_st_14d_act/sum'] = new_df['Amount_cn_st_actual']/new_df['Amount_cn_st_14d_sum']\n",
    "\n",
    "new_df = new_df.join(df2.groupby(['Cardnum','Merch state'])['Amount'].rolling(window = '30D', closed = 'left').mean(), on = ['Cardnum','Merch state','Date'], rsuffix = '_cn_st_30d_avg')\n",
    "new_df = new_df.join(df2.groupby(['Cardnum','Merch state'])['Amount'].rolling(window = '30D', closed = 'left').sum(), on = ['Cardnum','Merch state','Date'], rsuffix = '_cn_st_30d_sum')\n",
    "new_df['Amount_cn_st_30d_act-avg'] = new_df['Amount_cn_st_actual'] - new_df['Amount_cn_st_30d_avg']\n",
    "new_df['Amount_cn_st_30d_act/avg'] = new_df['Amount_cn_st_actual']/new_df['Amount_cn_st_30d_avg']\n",
    "new_df['Amount_cn_st_30d_act/sum'] = new_df['Amount_cn_st_actual']/new_df['Amount_cn_st_30d_sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### frequency variables\n",
    "new_df['Date'] = new_df['Date'].astype('datetime64[ns]')\n",
    "\n",
    "# Cardnum\n",
    "df3 = df.set_index('Date').groupby(['Cardnum'])['Recnum'].rolling(window = '1D', closed = 'left').count().to_frame().reset_index().drop_duplicates(subset = ['Cardnum','Date'],keep = 'first')\n",
    "new_df = new_df.merge(df3.rename(columns = {'Recnum':'Freq_cn_1d'}), on = ['Cardnum','Date'])\n",
    "\n",
    "df3 = df.set_index('Date').groupby(['Cardnum'])['Recnum'].rolling(window = '4D', closed = 'left').count().to_frame().reset_index().drop_duplicates(subset = ['Cardnum','Date'],keep = 'first')\n",
    "new_df = new_df.merge(df3.rename(columns = {'Recnum':'Freq_cn_4d'}), on = ['Cardnum','Date'])\n",
    "\n",
    "df3 = df.set_index('Date').groupby(['Cardnum'])['Recnum'].rolling(window = '7D', closed = 'left').count().to_frame().reset_index().drop_duplicates(subset = ['Cardnum','Date'],keep = 'first')\n",
    "new_df = new_df.merge(df3.rename(columns = {'Recnum':'Freq_cn_7d'}), on = ['Cardnum','Date'])\n",
    "\n",
    "df3 = df.set_index('Date').groupby(['Cardnum'])['Recnum'].rolling(window = '14D', closed = 'left').count().to_frame().reset_index().drop_duplicates(subset = ['Cardnum','Date'],keep = 'first')\n",
    "new_df = new_df.merge(df3.rename(columns = {'Recnum':'Freq_cn_14d'}), on = ['Cardnum','Date'])\n",
    "\n",
    "df3 = df.set_index('Date').groupby(['Cardnum'])['Recnum'].rolling(window = '30D', closed = 'left').count().to_frame().reset_index().drop_duplicates(subset = ['Cardnum','Date'],keep = 'first')\n",
    "new_df = new_df.merge(df3.rename(columns = {'Recnum':'Freq_cn_30d'}), on = ['Cardnum','Date'])\n",
    "\n",
    "# Merchnum\n",
    "df3 = df.set_index('Date').groupby(['Merchnum'])['Recnum'].rolling(window = '1D', closed = 'left').count().to_frame().reset_index().drop_duplicates(subset = ['Merchnum','Date'],keep = 'first')\n",
    "new_df = new_df.merge(df3.rename(columns = {'Recnum':'Freq_mn_1d'}), on = ['Merchnum','Date'])\n",
    "\n",
    "df3 = df.set_index('Date').groupby(['Merchnum'])['Recnum'].rolling(window = '4D', closed = 'left').count().to_frame().reset_index().drop_duplicates(subset = ['Merchnum','Date'],keep = 'first')\n",
    "new_df = new_df.merge(df3.rename(columns = {'Recnum':'Freq_mn_4d'}), on = ['Merchnum','Date'])\n",
    "\n",
    "df3 = df.set_index('Date').groupby(['Merchnum'])['Recnum'].rolling(window = '7D', closed = 'left').count().to_frame().reset_index().drop_duplicates(subset = ['Merchnum','Date'],keep = 'first')\n",
    "new_df = new_df.merge(df3.rename(columns = {'Recnum':'Freq_mn_7d'}), on = ['Merchnum','Date'])\n",
    "\n",
    "df3 = df.set_index('Date').groupby(['Merchnum'])['Recnum'].rolling(window = '14D', closed = 'left').count().to_frame().reset_index().drop_duplicates(subset = ['Merchnum','Date'],keep = 'first')\n",
    "new_df = new_df.merge(df3.rename(columns = {'Recnum':'Freq_mn_14d'}), on = ['Merchnum','Date'])\n",
    "\n",
    "df3 = df.set_index('Date').groupby(['Merchnum'])['Recnum'].rolling(window = '30D', closed = 'left').count().to_frame().reset_index().drop_duplicates(subset = ['Merchnum','Date'],keep = 'first')\n",
    "new_df = new_df.merge(df3.rename(columns = {'Recnum':'Freq_mn_30d'}), on = ['Merchnum','Date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cardnum & Merchnum\n",
    "df3 = df.set_index('Date').groupby(['Cardnum','Merchnum'])['Recnum'].rolling(window = '1D', closed = 'left').count().to_frame().reset_index().drop_duplicates(subset = ['Cardnum','Merchnum','Date'],keep = 'first')\n",
    "new_df = new_df.merge(df3.rename(columns = {'Recnum':'Freq_cn_mn_1d'}), on = ['Cardnum','Merchnum','Date'])\n",
    "\n",
    "df3 = df.set_index('Date').groupby(['Cardnum','Merchnum'])['Recnum'].rolling(window = '4D', closed = 'left').count().to_frame().reset_index().drop_duplicates(subset = ['Cardnum','Merchnum','Date'],keep = 'first')\n",
    "new_df = new_df.merge(df3.rename(columns = {'Recnum':'Freq_cn_mn_4d'}), on = ['Cardnum','Merchnum','Date'])\n",
    "\n",
    "df3 = df.set_index('Date').groupby(['Cardnum','Merchnum'])['Recnum'].rolling(window = '7D', closed = 'left').count().to_frame().reset_index().drop_duplicates(subset = ['Cardnum','Merchnum','Date'],keep = 'first')\n",
    "new_df = new_df.merge(df3.rename(columns = {'Recnum':'Freq_cn_mn_7d'}), on = ['Cardnum','Merchnum','Date'])\n",
    "\n",
    "df3 = df.set_index('Date').groupby(['Cardnum','Merchnum'])['Recnum'].rolling(window = '14D', closed = 'left').count().to_frame().reset_index().drop_duplicates(subset = ['Cardnum','Merchnum','Date'],keep = 'first')\n",
    "new_df = new_df.merge(df3.rename(columns = {'Recnum':'Freq_cn_mn_14d'}), on = ['Cardnum','Merchnum','Date'])\n",
    "\n",
    "df3 = df.set_index('Date').groupby(['Cardnum','Merchnum'])['Recnum'].rolling(window = '30D', closed = 'left').count().to_frame().reset_index().drop_duplicates(subset = ['Cardnum','Merchnum','Date'],keep = 'first')\n",
    "new_df = new_df.merge(df3.rename(columns = {'Recnum':'Freq_cn_mn_30d'}), on = ['Cardnum','Merchnum','Date'])\n",
    "\n",
    "# Cardnum & Merch zip\n",
    "df3 = df.set_index('Date').groupby(['Cardnum','Merch zip'])['Recnum'].rolling(window = '1D', closed = 'left').count().to_frame().reset_index().drop_duplicates(subset = ['Cardnum','Merch zip','Date'],keep = 'first')\n",
    "new_df = new_df.merge(df3.rename(columns = {'Recnum':'Freq_cn_zip_1d'}), on = ['Cardnum','Merch zip','Date'])\n",
    "\n",
    "df3 = df.set_index('Date').groupby(['Cardnum','Merch zip'])['Recnum'].rolling(window = '4D', closed = 'left').count().to_frame().reset_index().drop_duplicates(subset = ['Cardnum','Merch zip','Date'],keep = 'first')\n",
    "new_df = new_df.merge(df3.rename(columns = {'Recnum':'Freq_cn_zip_4d'}), on = ['Cardnum','Merch zip','Date'])\n",
    "\n",
    "df3 = df.set_index('Date').groupby(['Cardnum','Merch zip'])['Recnum'].rolling(window = '7D', closed = 'left').count().to_frame().reset_index().drop_duplicates(subset = ['Cardnum','Merch zip','Date'],keep = 'first')\n",
    "new_df = new_df.merge(df3.rename(columns = {'Recnum':'Freq_cn_zip_7d'}), on = ['Cardnum','Merch zip','Date'])\n",
    "\n",
    "df3 = df.set_index('Date').groupby(['Cardnum','Merch zip'])['Recnum'].rolling(window = '14D', closed = 'left').count().to_frame().reset_index().drop_duplicates(subset = ['Cardnum','Merch zip','Date'],keep = 'first')\n",
    "new_df = new_df.merge(df3.rename(columns = {'Recnum':'Freq_cn_zip_14d'}), on = ['Cardnum','Merch zip','Date'])\n",
    "\n",
    "df3 = df.set_index('Date').groupby(['Cardnum','Merch zip'])['Recnum'].rolling(window = '30D', closed = 'left').count().to_frame().reset_index().drop_duplicates(subset = ['Cardnum','Merch zip','Date'],keep = 'first')\n",
    "new_df = new_df.merge(df3.rename(columns = {'Recnum':'Freq_cn_zip_30d'}), on = ['Cardnum','Merch zip','Date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cardnum & Merch state\n",
    "df3 = df.set_index('Date').groupby(['Cardnum','Merch state'])['Recnum'].rolling(window = '1D', closed = 'left').count().to_frame().reset_index().drop_duplicates(subset = ['Cardnum','Merch state','Date'],keep = 'first')\n",
    "new_df = new_df.merge(df3.rename(columns = {'Recnum':'Freq_cn_st_1d'}), on = ['Cardnum','Merch state','Date'])\n",
    "\n",
    "df3 = df.set_index('Date').groupby(['Cardnum','Merch state'])['Recnum'].rolling(window = '4D', closed = 'left').count().to_frame().reset_index().drop_duplicates(subset = ['Cardnum','Merch state','Date'],keep = 'first')\n",
    "new_df = new_df.merge(df3.rename(columns = {'Recnum':'Freq_cn_st_4d'}), on = ['Cardnum','Merch state','Date'])\n",
    "\n",
    "df3 = df.set_index('Date').groupby(['Cardnum','Merch state'])['Recnum'].rolling(window = '7D', closed = 'left').count().to_frame().reset_index().drop_duplicates(subset = ['Cardnum','Merch state','Date'],keep = 'first')\n",
    "new_df = new_df.merge(df3.rename(columns = {'Recnum':'Freq_cn_st_7d'}), on = ['Cardnum','Merch state','Date'])\n",
    "\n",
    "df3 = df.set_index('Date').groupby(['Cardnum','Merch state'])['Recnum'].rolling(window = '14D', closed = 'left').count().to_frame().reset_index().drop_duplicates(subset = ['Cardnum','Merch state','Date'],keep = 'first')\n",
    "new_df = new_df.merge(df3.rename(columns = {'Recnum':'Freq_cn_st_14d'}), on = ['Cardnum','Merch state','Date'])\n",
    "\n",
    "df3 = df.set_index('Date').groupby(['Cardnum','Merch state'])['Recnum'].rolling(window = '30D', closed = 'left').count().to_frame().reset_index().drop_duplicates(subset = ['Cardnum','Merch state','Date'],keep = 'first')\n",
    "new_df = new_df.merge(df3.rename(columns = {'Recnum':'Freq_cn_st_30d'}), on = ['Cardnum','Merch state','Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(new_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Days since expert variables\n",
    "df3 = df.set_index('Date').groupby(['Cardnum'])['Recnum'].rolling(window = '1D', closed = 'left').count().to_frame().reset_index().drop_duplicates(subset = ['Cardnum','Date'])\n",
    "df3['Date_lag'] = df3.groupby('Cardnum')['Date'].shift(1)\n",
    "df3['last_cn'] = (df3['Date'] - df3['Date_lag']).dt.days\n",
    "new_df = new_df.merge(df3[['Cardnum','Date','last_cn']])\n",
    "\n",
    "df3 = df.set_index('Date').groupby(['Merchnum'])['Recnum'].rolling(window = '1D', closed = 'left').count().to_frame().reset_index().drop_duplicates(subset = ['Merchnum','Date'])\n",
    "df3['Date_lag'] = df3.groupby('Merchnum')['Date'].shift(1)\n",
    "df3['last_mn'] = (df3['Date'] - df3['Date_lag']).dt.days\n",
    "new_df = new_df.merge(df3[['Merchnum','Date','last_mn']])\n",
    "\n",
    "df3 = df.set_index('Date').groupby(['Cardnum','Merchnum'])['Recnum'].rolling(window = '1D', closed = 'left').count().to_frame().reset_index().drop_duplicates(subset = ['Cardnum','Merchnum','Date'])\n",
    "df3['Date_lag'] = df3.groupby(['Cardnum','Merchnum'])['Date'].shift(1)\n",
    "df3['last_cn_mn'] = (df3['Date'] - df3['Date_lag']).dt.days\n",
    "new_df = new_df.merge(df3[['Cardnum','Merchnum','Date','last_cn_mn']])\n",
    "\n",
    "df3 = df.set_index('Date').groupby(['Cardnum','Merch zip'])['Recnum'].rolling(window = '1D', closed = 'left').count().to_frame().reset_index().drop_duplicates(subset = ['Cardnum','Merch zip','Date'])\n",
    "df3['Date_lag'] = df3.groupby(['Cardnum','Merch zip'])['Date'].shift(1)\n",
    "df3['last_cn_zip'] = (df3['Date'] - df3['Date_lag']).dt.days\n",
    "new_df = new_df.merge(df3[['Cardnum','Merch zip','Date','last_cn_zip']])\n",
    "\n",
    "df3 = df.set_index('Date').groupby(['Cardnum','Merch state'])['Recnum'].rolling(window = '1D', closed = 'left').count().to_frame().reset_index().drop_duplicates(subset = ['Cardnum','Merch state','Date'])\n",
    "df3['Date_lag'] = df3.groupby(['Cardnum','Merch state'])['Date'].shift(1)\n",
    "df3['last_cn_st'] = (df3['Date'] - df3['Date_lag']).dt.days\n",
    "new_df = new_df.merge(df3[['Cardnum','Merch state','Date','last_cn_st']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Velocity deviation\n",
    "new_df['velo_cn_avg7d'] = new_df['Freq_cn_1d']/(new_df['Freq_cn_7d']/7)\n",
    "new_df['velo_cn_avg14d'] = new_df['Freq_cn_1d']/(new_df['Freq_cn_14d']/14)\n",
    "new_df['velo_cn_avg30d'] = new_df['Freq_cn_1d']/(new_df['Freq_cn_30d']/30)\n",
    "new_df['velo_mn_avg7d'] = new_df['Freq_mn_1d']/(new_df['Freq_mn_7d']/7)\n",
    "new_df['velo_mn_avg14d'] = new_df['Freq_mn_1d']/(new_df['Freq_mn_14d']/14)\n",
    "new_df['velo_mn_avg30d'] = new_df['Freq_mn_1d']/(new_df['Freq_mn_30d']/30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill na with 0\n",
    "new_df = new_df.fillna(0)\n",
    "\n",
    "# export amount expert variables\n",
    "new_df.to_csv('AmountVariables.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.read_csv('AmountVariables.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add a dummy variable as for sanity checks\n",
    "new_df['random'] = np.random.randint(0, 100, new_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate ks score for all variables\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "ks_df = pd.DataFrame()\n",
    "# ks_df['col'] = list(new_df)[10,:]\n",
    "ks_df['col'] = list(new_df[new_df.columns[~new_df.columns.isin(['Amount_cn_actual','Amount_mn_actual','Amount_cn_mn_actual','Amount_cn_zip_actual','Amount_cn_st_actual'])]])[10:]\n",
    "\n",
    "ks_df['p_value'] = ''\n",
    "ks_df['ks_score'] = ''\n",
    "for i in range(len(ks_df)):\n",
    "    ks_df['ks_score'][i] = ks_2samp(new_df[new_df['Fraud'] == 0][ks_df['col'][i]], new_df[new_df['Fraud'] == 1][ks_df['col'][i]])[0]\n",
    "    ks_df['p_value'][i] = ks_2samp(new_df[new_df['Fraud'] == 0][ks_df['col'][i]], new_df[new_df['Fraud'] == 1][ks_df['col'][i]])[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate fraud detection rate for all variables\n",
    "def fdr_cal(row):\n",
    "    col_name = row['col']\n",
    "    df_temp1 = new_df.sort_values(col_name, ascending = False)\n",
    "    df_temp_top1 = df_temp1[:round(len(new_df)*0.03)]\n",
    "    fdr1 = sum(df_temp_top1['Fraud'])/sum(df_temp1['Fraud'])\n",
    "    df_temp2 = new_df.sort_values(col_name, ascending = True)\n",
    "    df_temp_top2 = df_temp2[:round(len(new_df)*0.03)]\n",
    "    fdr2 = sum(df_temp_top2['Fraud'])/sum(df_temp2['Fraud'])\n",
    "    return max(fdr1, fdr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_df['fdr'] = ks_df.apply(lambda row: fdr_cal(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sort variables by fdr\n",
    "fdr_sorted_list = ks_df.sort_values('fdr', ascending = False)\n",
    "# fdr_sorted_list.to_pickle('fdr_sorted_list.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sort variables by ks_score\n",
    "ks_score_sorted_list = ks_df.sort_values('ks_score', ascending = False)\n",
    "# ks_score_sorted_list.to_pickle('ks_score_sorted_list.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ks_df['ks_rank'] = ks_df['ks_score'].rank(ascending = False)\n",
    "# ks_df['fdr_rank'] = ks_df['fdr'].rank(ascending = False)\n",
    "# ks_df['avg_rank'] = ks_df['ks_rank'] * 0.6 + ks_df['fdr_rank'] * 0.4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Divide into OOT and Train/Test\n",
    "\n",
    "## Standardizes in-place\n",
    "def standardize(df, colnames):\n",
    "    for col in colnames:\n",
    "        tmp_sd=df[col].std()\n",
    "        tmp_mean=df[col].mean()\n",
    "        df[col] = (df[col]-tmp_mean) / tmp_sd\n",
    "standardize(new_df,list(new_df)[11:])\n",
    "\n",
    "int_df = new_df[new_df[\"Date\"] < new_df[\"Date\"][84461]] \n",
    "oot_df = new_df[new_df[\"Date\"] > new_df[\"Date\"][84288]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate ks score for all variables\n",
    "def fdr_cal(row, df):\n",
    "    col_name = row['col']\n",
    "    df_temp = df.sort_values(col_name, ascending = False)\n",
    "    df_temp_top = df_temp[:round(len(df)*0.03)]\n",
    "    df_temp_bot = df_temp.tail(round(len(df)*0.03))\n",
    "    top_fdr= sum(df_temp_top['Fraud'])/sum(df_temp['Fraud'])\n",
    "    bot_fdr= sum(df_temp_bot['Fraud'])/sum(df_temp['Fraud'])\n",
    "#     print(str(top_fdr)+ \" \"+str(bot_fdr))\n",
    "    return(max(top_fdr, bot_fdr))\n",
    "\n",
    "def ks_fdr(df):\n",
    "    ks_df = pd.DataFrame()\n",
    "    ks_df['col'] = list(df)[10:]\n",
    "    ks_df['p_value'] = ''\n",
    "    ks_df['ks_score'] = ''\n",
    "    # Calc KS\n",
    "    for i in range(len(ks_df)):\n",
    "        ks_df['ks_score'][i] = ks_2samp(df[df['Fraud'] == 0][ks_df['col'][i]], df[df['Fraud'] == 1][ks_df['col'][i]])[0]\n",
    "        ks_df['p_value'][i] = ks_2samp(df[df['Fraud'] == 0][ks_df['col'][i]], df[df['Fraud'] == 1][ks_df['col'][i]])[1]\n",
    "    # Calc FDR\n",
    "    ks_df['fdr'] = ks_df.apply(lambda row: fdr_cal(row, df), axis=1)\n",
    "    return ks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ks_fdr_int = ks_fdr(int_df)\n",
    "ks_fdr_oot = ks_fdr(oot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_fdr_int['ks_rank'] = ks_fdr_int['ks_score'].rank(ascending = False)\n",
    "ks_fdr_int['fdr_rank'] = ks_fdr_int['fdr'].rank(ascending = False)\n",
    "ks_fdr_int['avg_rank'] = ks_fdr_int['ks_rank'] * 0.6 + ks_fdr_int['fdr_rank'] * 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                          Fraud\n",
       "79          Amount_cn_zip_actual\n",
       "53           Amount_cn_mn_actual\n",
       "105          Amount_cn_st_actual\n",
       "27              Amount_mn_actual\n",
       "1               Amount_cn_actual\n",
       "24         Amount_cn_30d_act-avg\n",
       "50         Amount_mn_30d_act-avg\n",
       "19         Amount_cn_14d_act-avg\n",
       "45         Amount_mn_14d_act-avg\n",
       "40          Amount_mn_7d_act-avg\n",
       "14          Amount_cn_7d_act-avg\n",
       "117          Amount_cn_st_7d_sum\n",
       "35          Amount_mn_4d_act-avg\n",
       "112          Amount_cn_st_4d_sum\n",
       "111          Amount_cn_st_4d_avg\n",
       "86          Amount_cn_zip_4d_sum\n",
       "18             Amount_cn_14d_sum\n",
       "85          Amount_cn_zip_4d_avg\n",
       "13              Amount_cn_7d_sum\n",
       "122         Amount_cn_st_14d_sum\n",
       "59           Amount_cn_mn_4d_avg\n",
       "128     Amount_cn_st_30d_act-avg\n",
       "60           Amount_cn_mn_4d_sum\n",
       "116          Amount_cn_st_7d_avg\n",
       "91          Amount_cn_zip_7d_sum\n",
       "8               Amount_cn_4d_sum\n",
       "7               Amount_cn_4d_avg\n",
       "51         Amount_mn_30d_act/avg\n",
       "127         Amount_cn_st_30d_sum\n",
       "                 ...            \n",
       "100        Amount_cn_zip_30d_avg\n",
       "52         Amount_mn_30d_act/sum\n",
       "101        Amount_cn_zip_30d_sum\n",
       "69          Amount_cn_mn_14d_avg\n",
       "123     Amount_cn_st_14d_act-avg\n",
       "3               Amount_cn_1d_sum\n",
       "2               Amount_cn_1d_avg\n",
       "75          Amount_cn_mn_30d_sum\n",
       "47         Amount_mn_14d_act/sum\n",
       "43             Amount_mn_14d_avg\n",
       "20         Amount_cn_14d_act/avg\n",
       "41          Amount_mn_7d_act/avg\n",
       "74          Amount_cn_mn_30d_avg\n",
       "38              Amount_mn_7d_avg\n",
       "80          Amount_cn_zip_1d_avg\n",
       "81          Amount_cn_zip_1d_sum\n",
       "39              Amount_mn_7d_sum\n",
       "102    Amount_cn_zip_30d_act-avg\n",
       "113      Amount_cn_st_4d_act-avg\n",
       "118      Amount_cn_st_7d_act-avg\n",
       "28              Amount_mn_1d_avg\n",
       "29              Amount_mn_1d_sum\n",
       "48             Amount_mn_30d_avg\n",
       "76      Amount_cn_mn_30d_act-avg\n",
       "55           Amount_cn_mn_1d_sum\n",
       "54           Amount_cn_mn_1d_avg\n",
       "21         Amount_cn_14d_act/sum\n",
       "42          Amount_mn_7d_act/sum\n",
       "152                Freq_cn_st_4d\n",
       "36          Amount_mn_4d_act/avg\n",
       "Name: col, Length: 80, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_features = ks_fdr_int.sort_values(\"avg_rank\")[\"col\"].head(80)\n",
    "top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "estimator = LogisticRegression(solver=\"liblinear\", max_iter=300)\n",
    "selector = RFE(estimator, 20, step=1)\n",
    "selector = selector.fit(int_df[top_features], int_df[\"Fraud\"])\n",
    "final_features=top_features[selector.support_]\n",
    "selected_df = pd.DataFrame(int_df, columns=list(final_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_df.to_csv(\"selected_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_df=pd.read_csv(\"selected_df.csv\")\n",
    "\n",
    "train, test = train_test_split(selected_df, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(oot_df[\"Fraud\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 100 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 14.4min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 80.0min\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed: 83.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 1500,\n",
       " 'min_samples_split': 2,\n",
       " 'min_samples_leaf': 2,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_depth': 20,\n",
       " 'criterion': 'entropy',\n",
       " 'bootstrap': True}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### --------- Rogers Models RF and SVM ------------\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "features=['Amount_cn_st_actual', 'Amount_mn_30d_act-avg', 'Amount_cn_14d_act-avg',\n",
    " 'Amount_mn_7d_act-avg', 'Amount_cn_7d_act-avg', 'Amount_cn_st_7d_sum',\n",
    " 'Amount_cn_st_4d_avg', 'Amount_cn_14d_sum', 'Amount_cn_st_30d_act-avg',\n",
    " 'Amount_cn_st_7d_avg', 'Amount_mn_30d_act/avg', 'Amount_cn_30d_act/avg',\n",
    " 'Amount_cn_mn_7d_sum', 'Amount_cn_30d_sum', 'Amount_cn_30d_act/sum',\n",
    " 'Amount_cn_zip_30d_avg', 'Amount_cn_zip_30d_sum', 'Amount_cn_st_7d_act-avg',\n",
    " 'Amount_mn_1d_avg']\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "## RF Hyperparams\n",
    "max_depth = [5,10,20,30]\n",
    "max_depth.append(None)\n",
    "rf_grid = {'n_estimators': [100, 500,1500],\n",
    "           'max_features': ['auto', 'sqrt'],\n",
    "           'criterion':['gini','entropy'],\n",
    "           'max_depth': max_depth,\n",
    "           'min_samples_split': [2, 5, 10],\n",
    "           'min_samples_leaf':[2, 5, 10, 15],\n",
    "           'bootstrap': [True]}\n",
    "rf_best = {'n_estimators': [100],\n",
    "         'min_samples_split': [2],\n",
    "         'min_samples_leaf': [4],\n",
    "         'max_features': ['auto'],\n",
    "         'max_depth': [90],\n",
    "         'bootstrap': [True]}\n",
    "# print(rf_grid)\n",
    "rf_random = RandomizedSearchCV(estimator = rf, \n",
    "                               param_distributions = rf_grid, \n",
    "                               n_iter = 100, \n",
    "                               cv = 2,\n",
    "                               verbose=1, \n",
    "                               random_state=123, \n",
    "                               n_jobs = -1)\n",
    "rf_random.fit(train[list(train)[2:]], train[\"Fraud\"])\n",
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy\n",
      "0.99878201441282940.99672389974343790.9855627578078963\n",
      "auc\n",
      "0.94748447586128440.84770647926973440.5940358891633\n",
      "fdr\n",
      "1.0\n",
      "0.8932806324110671\n",
      "0.6514285714285715\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "def metrics_cal(mod):\n",
    "    train[\"preds\"]=\"\"\n",
    "    test[\"preds\"]=\"\"\n",
    "    oot_df[\"preds\"]=\"\"\n",
    "    \n",
    "    train[\"preds\"]=mod.predict(train[features])\n",
    "    test[\"preds\"]=mod.predict(test[features])\n",
    "    oot_df[\"preds\"]=mod.predict(oot_df[features])\n",
    "    \n",
    "    print(\"accuracy\")\n",
    "    ra_=metrics.accuracy_score(train[\"Fraud\"], train[\"preds\"])\n",
    "    ta_=metrics.accuracy_score(test[\"Fraud\"], test[\"preds\"])\n",
    "    oa_=metrics.accuracy_score(oot_df[\"Fraud\"], oot_df[\"preds\"])\n",
    "    print(str(ra_) + str(ta_) + str(oa_))\n",
    "    \n",
    "    print(\"auc\")\n",
    "    rc_=metrics.roc_auc_score(train[\"Fraud\"], train[\"preds\"])\n",
    "    tc_=metrics.roc_auc_score(test[\"Fraud\"], test[\"preds\"])\n",
    "    oc_=metrics.roc_auc_score(oot_df[\"Fraud\"], oot_df[\"preds\"])\n",
    "    print(str(rc_) + str(tc_) + str(oc_))\n",
    "    \n",
    "    train[\"preds\"]=mod.predict_proba(train[features])\n",
    "    test[\"preds\"]=mod.predict_proba(test[features])\n",
    "    oot_df[\"preds\"]=mod.predict_proba(oot_df[features])\n",
    "    \n",
    "    print(\"fdr\")\n",
    "    \n",
    "    for df in [train,test,oot_df]:\n",
    "        df_temp = df.sort_values(\"preds\", ascending = False)\n",
    "        df_temp_top = df_temp[:round(len(df)*0.03)]\n",
    "        df_temp_bot = df_temp.tail(round(len(df)*0.03))\n",
    "        top_fdr= sum(df_temp_top['Fraud'])/sum(df_temp['Fraud'])\n",
    "        bot_fdr= sum(df_temp_bot['Fraud'])/sum(df_temp['Fraud'])\n",
    "        print(max(top_fdr, bot_fdr))\n",
    "\n",
    "# rf_preds=rf.predict_proba(test[list(train)[2:]])[:,1]\n",
    "\n",
    "metrics_cal(rf_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC  \n",
    "svc = SVC(kernel=\"rbf\",probability=True)    \n",
    "svm_grid = {'kernel': [\"linear\", \"rbf\"],\n",
    "            'degree': [2, 3, 4],\n",
    "            'gamma': [\"auto\"],\n",
    "            'shrinking': [True,False],\n",
    "            'class_weight': [\"balanced\"]}\n",
    "\n",
    "svm_best= {'shrinking': [True],\n",
    "           'kernel': ['rbf'],\n",
    "           'gamma': ['auto'],\n",
    "           'degree': [2],\n",
    "           'class_weight': [\"balanced\"]}\n",
    "\n",
    "svm_random = RandomizedSearchCV(estimator = svc, \n",
    "                                param_distributions = svm_best,\n",
    "                                n_jobs=-1,\n",
    "                                n_iter = 50, \n",
    "                                cv = 3, \n",
    "                                verbose=1, \n",
    "                                random_state=123)\n",
    "\n",
    "svm_random.fit(train[features], train[\"Fraud\"])\n",
    "# svc.fit(train[features], train[\"Fraud\"])\n",
    "\n",
    "# svm_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy\n",
      "0.97939574381703150.9780540753897770.9698487527008446\n",
      "auc\n",
      "0.93925981863462060.91653140544862640.6983285985524418\n",
      "fdr\n",
      "0.8982511923688394\n",
      "0.8537549407114624\n",
      "0.41714285714285715\n"
     ]
    }
   ],
   "source": [
    "metrics_cal(svm_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
